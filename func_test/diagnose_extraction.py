#!/usr/bin/env python3
"""
è¯Šæ–­æå–æµç¨‹ï¼šæ£€æŸ¥ Web Scraping â†’ Ranking â†’ Extraction å„é˜¶æ®µ
"""

import os
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from dotenv import load_dotenv
load_dotenv()

from config_manager import get_config

# æ£€æŸ¥é…ç½®
cfg = get_config()

print("="*80)
print("ğŸ” æå–æµç¨‹è¯Šæ–­")
print("="*80)

print("\n1ï¸âƒ£ Web Scraping é…ç½®æ£€æŸ¥")
print("-"*80)
web_scraping_enabled = cfg.get('ir_rag.web_scraping.enabled', False)
max_pages = cfg.get('ir_rag.web_scraping.max_pages', 0)
print(f"enabled: {web_scraping_enabled}")
print(f"max_pages: {max_pages}")

if not web_scraping_enabled:
    print("âŒ é—®é¢˜ï¼šWeb scraping æœªå¯ç”¨ï¼è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåªæœ‰ SERP_SNIPPET")
    print("   è§£å†³ï¼šåœ¨ config.yaml ä¸­è®¾ç½® ir_rag.web_scraping.enabled: true")
else:
    print("âœ… Web scraping å·²å¯ç”¨")

print("\n2ï¸âƒ£ Extraction é…ç½®æ£€æŸ¥")
print("-"*80)
snippet_threshold = cfg.get('ir_rag.extraction.snippet_confidence_threshold', 0.75)
max_passages = cfg.get('ir_rag.content.max_passages_per_task', 3)
print(f"snippet_confidence_threshold: {snippet_threshold}")
print(f"max_passages_per_task: {max_passages}")

if snippet_threshold > 0.7:
    print("âš ï¸  æç¤ºï¼šé˜ˆå€¼è¾ƒé«˜ (>0.7)ï¼Œå¯èƒ½å¯¼è‡´å¾ˆå¤šä»»åŠ¡è¿›å…¥ Stage 2")
    print("   å»ºè®®ï¼šé™ä½åˆ° 0.65 å¯ä»¥è®©æ›´å¤šä»»åŠ¡åœ¨ Stage 1 å°±æ»¡æ„")

if max_passages < 5:
    print("âš ï¸  æç¤ºï¼šmax_passages è¾ƒå°‘ï¼Œå¯èƒ½é”™è¿‡å…³é”®ä¿¡æ¯")
    print("   å»ºè®®ï¼šå¢åŠ åˆ° 6 å¯ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡")

print("\n3ï¸âƒ£ Search é…ç½®æ£€æŸ¥")
print("-"*80)
max_results = cfg.get('ir_rag.search.max_results', 10)
providers = cfg.get('ir_rag.search.provider', 'serpapi')
print(f"max_results: {max_results}")
print(f"provider: {providers}")

print("\n4ï¸âƒ£ Ranker é…ç½®æ£€æŸ¥")
print("-"*80)
ranking_algo = cfg.get('ir_rag.ranking.algorithm', 'keyword_matching')
entity_weight = cfg.get('ir_rag.ranking.entity_weight', 3.5)
keyword_weight = cfg.get('ir_rag.ranking.keyword_weight', 1.0)
print(f"algorithm: {ranking_algo}")
print(f"entity_weight: {entity_weight}")
print(f"keyword_weight: {keyword_weight}")

print("\n" + "="*80)
print("ğŸ“‹ å»ºè®®ä¿®æ”¹")
print("="*80)

recommendations = []

if not web_scraping_enabled:
    recommendations.append({
        "priority": "ğŸ”´ P0 - æœ€é«˜ä¼˜å…ˆçº§",
        "issue": "Web scraping æœªå¯ç”¨",
        "fix": "config.yaml ä¸­è®¾ç½®:\nir_rag:\n  web_scraping:\n    enabled: true\n    max_pages: 5"
    })

if snippet_threshold > 0.7:
    recommendations.append({
        "priority": "ğŸŸ¡ P1 - å»ºè®®ä¼˜åŒ–",
        "issue": f"snippet_confidence_threshold è¿‡é«˜ ({snippet_threshold})",
        "fix": "config.yaml ä¸­è®¾ç½®:\nir_rag:\n  extraction:\n    snippet_confidence_threshold: 0.65"
    })

if max_passages < 5:
    recommendations.append({
        "priority": "ğŸŸ¡ P1 - å»ºè®®ä¼˜åŒ–",
        "issue": f"max_passages_per_task è¾ƒå°‘ ({max_passages})",
        "fix": "config.yaml ä¸­è®¾ç½®:\nir_rag:\n  content:\n    max_passages_per_task: 6"
    })

if recommendations:
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. {rec['priority']}")
        print(f"   é—®é¢˜ï¼š{rec['issue']}")
        print(f"   ä¿®å¤ï¼š{rec['fix']}")
else:
    print("\nâœ… é…ç½®çœ‹èµ·æ¥æ²¡æœ‰æ˜æ˜¾é—®é¢˜")
    print("   å¦‚æœæå–ä»ç„¶å¤±è´¥ï¼Œå¯èƒ½æ˜¯ï¼š")
    print("   1. Wikipedia é¡µé¢æ²¡æœ‰è¢«é€‰ä¸­ï¼ˆURL é€‰æ‹©é€»è¾‘é—®é¢˜ï¼‰")
    print("   2. Ranker ç»™ Wikipedia å†…å®¹çš„åˆ†æ•°å¤ªä½")
    print("   3. Wikipedia å†…å®¹ä¸­ç¡®å®æ²¡æœ‰éœ€è¦çš„ä¿¡æ¯")

print("\n" + "="*80)

