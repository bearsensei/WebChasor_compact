"""
Information Extractor Module
Extracts structured information from content passages using LLM and fallback methods.
"""

import os
import json
import re
import time
import asyncio
import hashlib
import logging
from typing import List, Dict, Any, Optional, TYPE_CHECKING
from dataclasses import dataclass, field, replace

from config_manager import get_config

# Import from planner module
from planner import PlanTask, ExtractionPlan

# Import from ranker module
from actions.ranker import ContentPassage

# Avoid circular imports for type hints only
if TYPE_CHECKING:
    from actions.ir_rag import SearchResult

# Setup logging
logger = logging.getLogger(__name__)


# ============================================================================
# Data Models
# ============================================================================

@dataclass
class ExtractedVariable:
    """Extracted information for a specific variable"""
    variable_name: str
    value: Any
    confidence: float
    provenance: List[str] = field(default_factory=list)  # Source URLs
    extraction_method: str = "llm"  # llm, regex, ner
    raw_passages: List[str] = field(default_factory=list)


# ============================================================================
# InformationExtractor Class
# ============================================================================

class InformationExtractor:
    """Component for extracting structured information from passages"""

    def __init__(self, llm_client=None, model_name: str = None):
        self.client = llm_client
        cfg = get_config()
        self.model_name = model_name or os.getenv("OPENAI_API_MODEL_AGENT_SYNTHESIZER", "gpt-3.5-turbo")
        self.small_model_name = cfg.get('models.extractor.small_model_name', self.model_name)
        self.small_max_tokens = cfg.get('models.extractor.small_max_tokens', 1000)
        self.small_temperature = cfg.get('models.extractor.small_temperature', 0.0)
        default_max_tokens = cfg.get('models.extractor.max_tokens', 4000)
        self.stage2_max_tokens = cfg.get('models.extractor.stage2_max_tokens', max(1500, default_max_tokens // 2))
        self.stage2_chunk_limit = cfg.get('ir_rag.extraction.stage2_chunk_limit', 3)
        self.snippet_confidence_threshold = cfg.get('ir_rag.extraction.snippet_confidence_threshold', 0.75)
        self.stage1_concurrency = cfg.get('ir_rag.extraction.stage1_concurrency', 5)
        self.stage2_concurrency = cfg.get('ir_rag.extraction.stage2_concurrency', 3)
        self._cache: Dict[str, ExtractedVariable] = {}

    def _looks_structured(self, text: str) -> bool:
        """Heuristic, format-agnostic detection of structure (markdown/html tables or explicit [TABLE]/[LIST])."""
        if not text:
            return False
        t = text.lower()
        # explicit structured markers or common table/list signals
        return (
            "[table]" in t or "[list]" in t or
            "<table" in t or "&lt;table" in t or
            "\n|" in text or "| " in text or
            re.search(r'^\s*[-*]\s+\S', text, flags=re.MULTILINE) is not None
        )

    def _pick_body_text(self, passage: 'ContentPassage', meta: Dict[str, Any], mode: str) -> str:
        """
        Prefer structure-preserving text when available.
        - If mode == 'snippet' and meta['snippet'] exists, use it (for SERP snippets).
        - If mode == 'mixed': use raw_text if it has structure, else use snippet if available.
        - Else prefer passage.raw_text (keeps tables/lists), fallback to passage.text.
        """
        if mode == "snippet" and meta.get('snippet') and passage.source_url == "SERP_SNIPPET":
            return str(meta['snippet'])
        
        if mode == "mixed":
            # For mixed mode: prioritize structured content from raw_text
            raw = passage.raw_text or ""
            if raw and self._looks_structured(raw):
                return raw
            # Fallback to snippet for SERP results, or text for others
            if passage.source_url == "SERP_SNIPPET" and meta.get('snippet'):
                return str(meta['snippet'])
            return raw or passage.text or ""
        
        return passage.raw_text or passage.text or ""

    def _summarize_for_cache(self, s: str) -> str:
        """Stable short fingerprint source for cache/dedupe (avoid huge keys)."""
        if not s:
            return ""
        s = s.strip()
        if len(s) <= 200:
            return s
        return s[:100] + s[-100:]
    
    async def extract_variables(
        self,
        plan: ExtractionPlan,
        ranked_passages: Dict[str, List[ContentPassage]],
        search_results: Optional[List['SearchResult']] = None,
        ctx: Optional[Any] = None
    ) -> Dict[str, ExtractedVariable]:
        """Two-stage extraction using fast snippets first, then deep passages."""

        task_count = len(plan.tasks_to_extract)
        print(f"[EXTRACTOR][Pipeline] Starting two-stage extraction for {task_count} variables")

        extracted_vars: Dict[str, ExtractedVariable] = {}
        unresolved: List[tuple] = []  # (task, passages, stage1_candidate)

        stage1_start = time.perf_counter()

        # Run Stage 1 concurrently for all tasks
        stage1_results = await self._run_stage1(plan.tasks_to_extract, ranked_passages, ctx)

        stage1_solved = 0
        for task, passages, stage1_result in stage1_results:
            if stage1_result:
                extracted_vars[task.variable_name] = stage1_result
                if stage1_result.confidence >= self.snippet_confidence_threshold:
                    stage1_solved += 1
                    print(
                        f"[EXTRACTOR][Stage1] Satisfied '{task.variable_name}' "
                        f"(confidence={stage1_result.confidence:.2f})"
                    )
                    continue
                else:
                    print(
                        f"[EXTRACTOR][Stage1] Tentative '{task.variable_name}' "
                        f"(confidence={stage1_result.confidence:.2f})"
                    )

            unresolved.append((task, passages, stage1_result))

        stage1_elapsed = time.perf_counter() - stage1_start
        print(
            f"[EXTRACTOR][Stage1] Solved {stage1_solved}/{task_count} variables "
            f"in {stage1_elapsed:.2f}s (concurrent)"
        )

        if not unresolved:
            return extracted_vars

        stage2_results = await self._run_stage2(unresolved, ctx)

        for task, result in stage2_results:
            prior = extracted_vars.get(task.variable_name)
            if prior is None or (result and result.confidence >= prior.confidence):
                extracted_vars[task.variable_name] = result

        threshold_map = {t.variable_name: t.confidence_threshold for t in plan.tasks_to_extract}
        solved_total = sum(
            1
            for name, value in extracted_vars.items()
            if value and value.confidence >= threshold_map.get(name, self.snippet_confidence_threshold)
        )
        print(
            f"[EXTRACTOR][Pipeline] Stage2 resolved {len(stage2_results)} tasks; "
            f"total confident extractions: {solved_total}/{task_count}"
        )

        return extracted_vars

    async def _run_stage1(
        self,
        tasks: List[PlanTask],
        ranked_passages: Dict[str, List[ContentPassage]],
        ctx: Optional[Any] = None
    ) -> List[tuple]:
        """
        Run Stage 1 extraction using batch mode for efficiency.
        Extracts all tasks in one LLM call with shared passages.
        Returns list of (task, passages, stage1_result) tuples.
        """
        if not tasks:
            return []

        # Try batch extraction first (more efficient)
        try:
            batch_results = await self._run_stage1_batch(tasks, ranked_passages, ctx)
            if batch_results:
                return batch_results
        except Exception as e:
            print(f"[EXTRACTOR][Stage1] Batch mode failed: {e}, falling back to individual extraction")
        
        # Fallback to individual extraction if batch fails
        return await self._run_stage1_individual(tasks, ranked_passages, ctx)
    
    async def _run_stage1_batch(
        self,
        tasks: List[PlanTask],
        ranked_passages: Dict[str, List[ContentPassage]],
        ctx: Optional[Any] = None
    ) -> List[tuple]:
        """
        Batch extraction: extract all tasks in one LLM call.
        More efficient and provides global context for deduplication.
        """
        if not tasks or not self.client:
            return None

        # Step 1: Merge and deduplicate passages from all tasks
        all_passages_dict = {}
        for task in tasks:
            passages = ranked_passages.get(task.variable_name, [])
            for p in passages:
                # Use URL as key for deduplication
                key = f"{p.source_url}::{p.text[:100]}"  # Include text prefix for better deduplication
                if key not in all_passages_dict or p.score > all_passages_dict[key].score:
                    all_passages_dict[key] = p

        # Step 2: Filter to snippets only and sort by score
        snippet_passages = [
            p for p in all_passages_dict.values()
            if p.source_url == "SERP_SNIPPET"
        ]
        snippet_passages.sort(key=lambda p: p.score, reverse=True)

        # Step 3: Limit to top snippets to avoid token overflow
        top_snippets = snippet_passages[:15]  # Top 15 most relevant snippets

        # Optionally add a couple of structure-rich passages (tables/lists) to help roster/list tasks.
        struct_candidates = [
            p for p in all_passages_dict.values()
            if p.source_url != "SERP_SNIPPET" and self._looks_structured(p.raw_text or p.text)
        ]
        struct_candidates.sort(key=lambda p: p.score, reverse=True)
        top_struct = struct_candidates[:2]

        shared_inputs = top_snippets + top_struct

        if not shared_inputs:
            print(f"[EXTRACTOR][Stage1-Batch] No passages available for batch extraction")
            return None

        print(f"[EXTRACTOR][Stage1-Batch] Extracting {len(tasks)} tasks from {len(shared_inputs)} shared passages (snippets + structured)")

        # Step 4: Batch extract all tasks
        batch_results = await self._batch_extract(tasks, shared_inputs, ctx, stage="stage1")

        # Step 5: Map results back to (task, passages, result) format
        results = []
        for task in tasks:
            passages = ranked_passages.get(task.variable_name, [])
            result = batch_results.get(task.variable_name)
            results.append((task, passages, result))

        return results
    
    async def _run_stage1_individual(
        self,
        tasks: List[PlanTask],
        ranked_passages: Dict[str, List[ContentPassage]],
        ctx: Optional[Any] = None
    ) -> List[tuple]:
        """
        Original individual extraction logic as fallback.
        """
        semaphore = asyncio.Semaphore(max(1, self.stage1_concurrency))
        
        async def process_task(task: PlanTask) -> tuple:
            passages = ranked_passages.get(task.variable_name, [])
            if not passages:
                print(f"[EXTRACTOR][Stage1] No passages for {task.variable_name}")
                return task, passages, None

            snippet_passages = [p for p in passages if p.source_url == "SERP_SNIPPET"]
            stage1_result = None

            if snippet_passages and self.client:
                async with semaphore:
                    stage1_result = await self._stage1_extract(task, snippet_passages[:5], ctx)

            return task, passages, stage1_result

        # Process all tasks concurrently
        results = await asyncio.gather(*[process_task(task) for task in tasks])
        return results

    async def _stage1_extract(self, task: PlanTask, snippet_passages: List[ContentPassage], ctx: Optional[Any] = None) -> Optional[ExtractedVariable]:
        """Fast snippet-first extraction using a lightweight model."""
        try:
            result = await self._llm_extract(
                task,
                snippet_passages,
                stage_label="llm-snippet",
                model_name=self.small_model_name,
                max_tokens=self.small_max_tokens,
                temperature=self.small_temperature,
                mode="snippet",
                allow_fallback=False,
                ctx=ctx
            )
            return result
        except Exception as exc:
            logger.error(f"[EXTRACTOR][STAGE1] Failed for {task.variable_name}: {exc}")
            return None
    
    async def _batch_extract(
        self,
        tasks: List[PlanTask],
        passages: List[ContentPassage],
        ctx: Optional[Any] = None,
        stage: str = "stage1"
    ) -> Dict[str, ExtractedVariable]:
        """
        Batch extract multiple tasks from shared passages in one LLM call.
        More efficient and provides global context for automatic deduplication.
        """
        if not tasks or not passages or not self.client:
            return {}
        
        cfg = get_config()
        
        # Step 1: Format passages as structured EVIDENCE
        # Use mixed mode: prefer raw_text for structured content, snippet for SERP snippets
        mode = "mixed"  # Allow _pick_body_text to intelligently choose
        combined_text, provenance, raw_texts = self._format_passages(passages, mode=mode)
        
        if not combined_text.strip():
            return {}
        
        # Step 2: Build PLAN_TASKS JSON (all tasks)
        plan_tasks_json = json.dumps([
            {
                "variable_name": task.variable_name,
                "fact": task.fact,
                "category": task.category
            }
            for task in tasks
        ], ensure_ascii=False)
        
        # Step 3: Detect language
        lang = self._detect_language(tasks[0].fact + " " + combined_text[:500])
        
        # Step 4: Extract time context
        time_rules = ""
        if ctx and hasattr(ctx, 'time_context') and ctx.time_context:
            tc = ctx.time_context
            if tc.window and tc.window[0]:
                target_date = tc.window[0].split('T')[0]
                if lang == "zh":
                    time_rules = f"\n\næ—¶é—´è§„åˆ™ï¼šç›®æ ‡æ—¥æœŸ = {target_date}ã€‚åªæ¥å—æ˜ç¡®æ ‡æ³¨æ­¤æ—¥æœŸçš„æ•°æ®ã€‚"
                else:
                    time_rules = f"\n\nTime Rule: Target date = {target_date}. Only accept data explicitly marked with this date."
        
        # Step 5: Build prompt
        if lang == "zh":
            system_msg = """ä½ æ˜¯ç»“æ„åŒ–æå–ä¸“å®¶ã€‚ä»EVIDENCEä¸­æå–æ‰€æœ‰PLAN_TASKSè¦æ±‚çš„äº‹å®ã€‚
è¾“å‡ºä¸¥æ ¼JSONï¼Œä¸çŒœæµ‹ã€‚å¦‚ç¼ºå¤±æˆ–å†²çªï¼Œæ ‡æ³¨"status":"unverified"æˆ–"conflict"ã€‚

æ‰¹é‡æå–è§„åˆ™ï¼š
- ä¸ºæ¯ä¸ªtaskè¿”å›ä¸€ä¸ªresult
- ä»æ‰€æœ‰EVIDENCEä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„ä¿¡æ¯
- å¦‚æœå¤šä¸ªtasksè¯­ä¹‰é‡å¤ï¼ˆå¦‚achievementså’Œnotable_factsï¼‰ï¼Œåˆå¹¶åˆ°ä¸€ä¸ªtaskä¸­ï¼Œå…¶ä»–è¿”å›null
- å¦‚æœæŸtaskæ²¡æœ‰ä¿¡æ¯ï¼Œè¿”å›"status":"unverified", "value":null

åˆ—è¡¨/åå•/è¡¨æ ¼æå–è§„åˆ™ï¼ˆå…³é”®ï¼‰ï¼š
- å½“taskè¦æ±‚æå–åå•ã€åˆ—è¡¨ã€æˆå‘˜ã€é¢†å¯¼äººã€å®˜å‘˜æ—¶ï¼Œå¿…é¡»æå–æ‰€æœ‰åå­—ï¼Œä¸è¦ä»…æ€»ç»“
- å¯»æ‰¾æ¨¡å¼ï¼š"XXXå±€é•¿ï¼šYYY"ã€"XXX: YYY (å¹´ä»½)"ã€è¡¨æ ¼æ ¼å¼[TABLE]...[/TABLE]
- å¦‚æœçœ‹åˆ°[TABLE]æ ‡è®°ï¼Œæå–**æ‰€æœ‰è¡Œ**çš„æ•°æ®
- **å¹´ä»½æ˜¯å¯é€‰çš„**ï¼šå¦‚æœè¡¨æ ¼/æ–‡æœ¬ä¸­æœ‰å¹´ä»½å°±é™„å¸¦ï¼Œæ²¡æœ‰å¹´ä»½ä¹Ÿç…§æ ·æå–å§“å+èŒä½
- æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–åˆ—è¡¨ï¼š
  * æœ‰å¹´ä»½ï¼š"èŒä½1ï¼šå§“å1ï¼ˆ2022å¹´7æœˆï¼‰; èŒä½2ï¼šå§“å2ï¼ˆ2020å¹´ï¼‰; ..."
  * æ— å¹´ä»½ï¼š"èŒä½1ï¼šå§“å1; èŒä½2ï¼šå§“å2; ..."
  * æ··åˆï¼š"èŒä½1ï¼šå§“å1ï¼ˆ2022å¹´ï¼‰; èŒä½2ï¼šå§“å2; èŒä½3ï¼šå§“å3ï¼ˆ2023å¹´ï¼‰; ..."
- ç¤ºä¾‹ï¼š
  âœ… æ­£ç¡®ï¼š"æ”¿å‹™å¸å¸é•·ï¼šé™³åœ‹åŸºï¼ˆ2022å¹´7æœˆï¼‰; è²¡æ”¿å¸å¸é•·ï¼šé™³èŒ‚æ³¢ï¼ˆ2017å¹´1æœˆï¼‰; å¾‹æ”¿å¸å¸é•·ï¼šæ—å®šåœ‹ï¼ˆ2022å¹´7æœˆï¼‰"
  âœ… æ­£ç¡®ï¼š"å…¬å‹™å“¡äº‹å‹™å±€ï¼šæ¥Šä½•è““èŒµ; æ”¿åˆ¶åŠå…§åœ°äº‹å‹™å±€ï¼šæ›¾åœ‹è¡; æ–‡åŒ–é«”è‚²åŠæ—…éŠå±€ï¼šç¾…æ·‘ä½©"ï¼ˆæ²¡æœ‰å¹´ä»½ä¹Ÿå¯ä»¥ï¼‰
  âŒ é”™è¯¯ï¼š"æœ‰ä¸‰ä½å¸é•·"ï¼ˆç¼ºå°‘å…·ä½“åå­—ï¼‰
  âŒ é”™è¯¯ï¼š"åŒ…æ‹¬æ”¿å‹™å¸ã€è²¡æ”¿å¸ã€å¾‹æ”¿å¸"ï¼ˆåªæœ‰èŒä½ï¼Œç¼ºå°‘äººåï¼‰
- å³ä½¿æ–‡æœ¬å¾ˆé•¿ï¼Œä¹Ÿè¦æå–æ‰€æœ‰æåˆ°çš„åå­—ï¼Œä¸è¦é—æ¼
- **é‡è¦**ï¼šå³ä½¿ä»»åŠ¡ååŒ…å« "with_years"ï¼Œä¹Ÿè¦æå–æ²¡æœ‰å¹´ä»½çš„æ¡ç›®

æ—¶é—´ä¿¡æ¯æå–è§„åˆ™ï¼ˆä»…é€‚ç”¨äºéåˆ—è¡¨ç±»ä»»åŠ¡ï¼‰ï¼š
- å¯¹äºå•ä¸€äº‹ä»¶/å¥–é¡¹/æˆå°±ï¼ˆéåˆ—è¡¨ï¼‰ï¼Œå°½é‡æå–å¹´ä»½
- å¹´ä»½å¿…é¡»ä¸äº‹ä»¶æ˜ç¡®å…³è”ï¼ˆåŒå¥æˆ–é‚»è¿‘Â±120å­—ç¬¦ï¼‰
- å¿½ç•¥é¡µé¢å…ƒæ•°æ®æ—¥æœŸï¼ˆ"æ›´æ–°äº"ã€"å‘å¸ƒäº"ï¼‰
- èŒä¸š/å¥–é¡¹æ ¼å¼ï¼š"Position (YYYY)" æˆ– "Position, started YYYY"
- å¦‚æœæ‰¾ä¸åˆ°å¹´ä»½ä½†äº‹å®å­˜åœ¨ï¼Œvalueæ­£å¸¸å¡«å†™ï¼Œnotesä¸­è¯´æ˜"å¹´ä»½æœªæ‰¾åˆ°"
- åŒºåˆ†äº‹ä»¶ç±»å‹ï¼šå½“é€‰/ä»»å‘½/å°±èŒ/å¸ä»»/è·å¥–ï¼Œæ¯ä¸ªéƒ½è¦æ ‡æ³¨å¹´ä»½
- **æ³¨æ„**ï¼šå¯¹äºåˆ—è¡¨/åå•ç±»ä»»åŠ¡ï¼Œå¹´ä»½æ˜¯å¯é€‰çš„ï¼ˆè§ä¸Šæ–¹åˆ—è¡¨æå–è§„åˆ™ï¼‰
"""
            prompt = f"""INPUT
PLAN_TASKS: {plan_tasks_json}
EVIDENCE: {combined_text}{time_rules}

OUTPUT JSON SCHEMA
{{
  "results": [
    {{
      "variable_name": "...", 
      "status": "ok"|"unverified"|"conflict",
      "value": "string (if temporal: include year as 'Position (YYYY)' or 'äº‹ä»¶ (YYYYå¹´)')",
      "source_ids": [1,2],
      "notes": "ç®€çŸ­åŸå› ï¼Œå¦‚æ¶‰åŠæ—¶é—´ä½†æœªæ‰¾åˆ°å¹´ä»½ï¼Œå¿…é¡»è¯´æ˜"
    }},
    ...
  ]
}}

è¦æ±‚ï¼š
1. ä¸ºPLAN_TASKSä¸­çš„æ¯ä¸ªtaskè¿”å›ä¸€ä¸ªresult
2. ä»EVIDENCEä¸­é€‰æ‹©æœ€ç›¸å…³çš„ä¿¡æ¯
3. ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœtaskæ¶‰åŠèŒä½/å¥–é¡¹/äº‹ä»¶ï¼Œvalueä¸­å¿…é¡»åŒ…å«å¹´ä»½ä¿¡æ¯
4. å¦‚æœå‘ç°tasksé‡å¤ï¼Œä¼˜å…ˆå¡«å……ç¬¬ä¸€ä¸ªï¼Œå…¶ä»–è¿”å›null
5. ä»…è¿”å›JSONï¼Œä¸è¦è§£é‡Šã€‚"""
        else:
            system_msg = """You are a Structured Extractor. Extract all PLAN_TASKS from EVIDENCE in one batch.
Output STRICT JSON only. Do NOT guess. If missing or conflicting, mark "status":"unverified" or "conflict".

Batch extraction rules:
- Return one result for each task
- Find the most relevant information from all EVIDENCE
- If multiple tasks are semantically duplicate (e.g. achievements and notable_facts), merge into one task, return null for others
- If no information for a task, return "status":"unverified", "value":null

List/Roster/Table Extraction Rules (CRITICAL):
- When task asks for lists, rosters, members, leaders, officials: extract ALL names, do NOT just summarize
- Look for patterns: "XXX Chief: YYY", "XXX: YYY (Year)", table formats [TABLE]...[/TABLE]
- If you see [TABLE] markers, extract **ALL rows** of data
- **Year is OPTIONAL**: If table/text has year info, include it; if no year, still extract name+position
- Format as structured list:
  * With year: "Position1: Name1 (July 2022); Position2: Name2 (2020); ..."
  * Without year: "Position1: Name1; Position2: Name2; ..."
  * Mixed: "Position1: Name1 (2022); Position2: Name2; Position3: Name3 (2023); ..."
- Examples:
  âœ… GOOD: "Chief Secretary: Chan Kwok-ki (July 2022); Financial Secretary: Paul Chan (Jan 2017); Secretary for Justice: Paul Lam (July 2022)"
  âœ… GOOD: "Civil Service Bureau: Ada Chung; Constitutional Affairs Bureau: Erick Tsang; Culture Bureau: Rosanna Law" (no year is OK)
  âŒ BAD: "There are three secretaries" (missing actual names)
  âŒ BAD: "Including Chief Secretary, Financial Secretary, and Secretary for Justice" (titles only, missing names)
- Even if text is long, extract ALL mentioned names, do not omit any
- **IMPORTANT**: Even if task name contains "with_years", extract entries WITHOUT year too

Temporal Information Extraction Rules (for non-list tasks only):
- For single events/awards/achievements (non-list), try to extract year
- Year must be explicitly linked to the event (same sentence or within Â±120 chars)
- Ignore page meta dates ("updated on", "published on", "last modified")
- Format for positions/awards: "Position (YYYY)" or "Position, started YYYY"
- If year not found but fact exists, fill value normally and note "year not found" in notes
- Distinguish event types: elected/appointed/assumed/resigned/awarded, each with year
- **Note**: For list/roster tasks, year is OPTIONAL (see list extraction rules above)
"""
            prompt = f"""INPUT
PLAN_TASKS: {plan_tasks_json}
EVIDENCE: {combined_text}{time_rules}

OUTPUT JSON SCHEMA
{{
  "results": [
    {{
      "variable_name": "...", 
      "status": "ok"|"unverified"|"conflict",
      "value": "string (if temporal: include year as 'Position (YYYY)' or 'Event in YYYY')",
      "source_ids": [1,2],
      "notes": "short reason, if temporal but year not found, must explain"
    }},
    ...
  ]
}}

Requirements:
1. Return one result for each task in PLAN_TASKS
2. Select most relevant information from EVIDENCE
3. CRITICAL: If task involves position/award/event, value MUST include year information
4. If tasks are duplicate, prioritize first one, return null for others
5. Return JSON only, no explanation."""
        
        # Step 6: Call LLM
        model_name = self.small_model_name if stage == "stage1" else self.model_name
        max_tokens = (self.small_max_tokens * 2) if stage == "stage1" else (self.stage2_max_tokens * 2)
        
        try:
            response = self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # Deterministic for batch extraction
                max_tokens=max_tokens,
                response_format={"type": "json_object"}
            )
        except Exception as api_error:
            print(f"[EXTRACTOR][BATCH] response_format not supported, retrying without it")
            response = self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=max_tokens
            )
        
        # Step 7: Parse results
        result_text = (response.choices[0].message.content or "").strip()
        if not result_text:
            print(f"[EXTRACTOR][BATCH] Empty LLM response")
            return {}
        
        try:
            result_data = json.loads(result_text)
        except json.JSONDecodeError as err:
            print(f"[EXTRACTOR][BATCH] JSON decode failed: {err}")
            print(f"[EXTRACTOR][BATCH] Raw response: {result_text[:200]}")
            return {}
        
        # Step 8: Map results back to ExtractedVariable format
        extracted_vars = {}
        results = result_data.get("results", [])
        
        for result in results:
            var_name = result.get("variable_name")
            if not var_name:
                continue
            
            status = result.get("status", "ok")
            value = result.get("value")
            notes = result.get("notes", "")
            
            # Calculate confidence based on status
            if status == "ok":
                confidence = 0.95
            elif status == "unverified":
                confidence = 0.60
            elif status == "conflict":
                confidence = 0.40
            else:
                confidence = 0.50
            
            # ğŸ”¥ FALLBACK: If extraction failed, use raw material
            extraction_method = f"batch-{stage}"
            if (not value or confidence < 0.7) and raw_texts:
                # Find the corresponding task for better context
                task_obj = next((t for t in tasks if t.variable_name == var_name), None)
                if task_obj:
                    raw_material = self._extract_relevant_raw_material(raw_texts, task_obj)
                    if raw_material and len(raw_material) > 50:
                        value = raw_material
                        confidence = 0.65
                        status = "raw_fallback"
                        notes = f"æ‰¹é‡æå–å¤±è´¥ï¼Œè¿”å›åŸå§‹ææ–™ã€‚åŸå› ï¼š{notes}"
                        extraction_method = "batch-raw-fallback"
                        print(f"[EXTRACTOR][BATCH-FALLBACK] {var_name}: using raw material ({len(raw_material)} chars)")
            
            # Map source_ids to provenance URLs
            source_ids = result.get("source_ids", [])
            selected_provenance = [provenance[i-1] for i in source_ids if 1 <= i <= len(provenance)]
            if not selected_provenance:
                selected_provenance = provenance[:3]  # Use first 3 as fallback
            
            extracted_vars[var_name] = ExtractedVariable(
                variable_name=var_name,
                value=value,
                confidence=confidence,
                provenance=selected_provenance,
                extraction_method=extraction_method,
                raw_passages=raw_texts[:3]  # Include sample passages
            )
            
            if cfg.is_decision_logging_enabled('ir_rag'):
                value_preview = str(value)[:50] if value else 'None'
                print(
                    f"[EXTRACTOR][BATCH] {var_name}: value={value_preview}... "
                    f"confidence={confidence:.2f} status={status}"
                )
        
        print(f"[EXTRACTOR][BATCH] Successfully extracted {len(extracted_vars)}/{len(tasks)} tasks")
        
        return extracted_vars

    async def _run_stage2(self, unresolved: List[tuple], ctx: Optional[Any] = None) -> List[tuple]:
        """Run focused deep extraction for unresolved variables concurrently."""
        if not unresolved:
            return []

        stage2_start = time.perf_counter()

        if not self.client:
            logger.warning("[EXTRACTOR][Stage2] No LLM client; using fallback extraction")
            results = []
            for task, passages, _ in unresolved:
                fallback = self._fallback_extract(task, passages)
                results.append((task, fallback))
            return results

        semaphore = asyncio.Semaphore(max(1, self.stage2_concurrency))
        tasks = [self._stage2_extract(task, passages, provisional, semaphore, ctx) for task, passages, provisional in unresolved]
        stage2_outputs = await asyncio.gather(*tasks)

        elapsed = time.perf_counter() - stage2_start
        solved = sum(1 for _, result in stage2_outputs if result and result.confidence >= 0.6)
        print(
            f"[EXTRACTOR][Stage2] Completed {len(stage2_outputs)} tasks in {elapsed:.2f}s "
            f"(confidence â‰¥0.6: {solved}, concurrent)"
        )

        return stage2_outputs

    async def _stage2_extract(
        self,
        task: PlanTask,
        passages: List[ContentPassage],
        provisional: Optional[ExtractedVariable],
        semaphore: asyncio.Semaphore,
        ctx: Optional[Any] = None
    ) -> tuple:
        """Run deep extraction with focused passages and caching."""

        focused_passages = self._prepare_stage2_passages(passages)
        cache_key = self._cache_key(task, focused_passages, "llm-deep")

        if cache_key in self._cache:
            cached = self._cache[cache_key]
            print(f"[EXTRACTOR][Stage2] Cache hit for {task.variable_name}")
            return task, replace(cached)

        focused_passages = focused_passages[:2000]
        async with semaphore:
            try:
                result = await self._llm_extract(
                    task,
                    focused_passages,
                    stage_label="llm-deep",
                    model_name=self.model_name,
                    max_tokens=self.stage2_max_tokens,
                    temperature=0.1,
                    mode="passage",
                    allow_fallback=True,
                    ctx=ctx
                )
                if result:
                    self._cache[cache_key] = result
                    return task, result
            except Exception as exc:
                logger.error(f"[EXTRACTOR][STAGE2] Failed for {task.variable_name}: {exc}")

        # Fallback if we reach here
        if provisional and provisional.confidence > 0:
            return task, provisional
        return task, self._fallback_extract(task, focused_passages or passages)

    def _prepare_stage2_passages(self, passages: List[ContentPassage]) -> List[ContentPassage]:
        """Select top passages from fetched pages and keep supporting snippets."""
        if not passages:
            return []

        primary = [p for p in passages if p.source_url != "SERP_SNIPPET"]
        secondary = [p for p in passages if p.source_url == "SERP_SNIPPET"]

        primary.sort(key=lambda p: p.score, reverse=True)
        selected = primary[: self.stage2_chunk_limit]

        if len(selected) < self.stage2_chunk_limit and secondary:
            need = self.stage2_chunk_limit - len(selected)
            selected.extend(secondary[:need])

        # Deduplicate by structure-preserving text (prefer raw_text), compact
        seen_text = set()
        unique_passages = []
        for passage in selected:
            base = passage.raw_text or passage.text or ""
            text_key = self._summarize_for_cache(base)
            if text_key in seen_text:
                continue
            seen_text.add(text_key)
            unique_passages.append(passage)

        return unique_passages

    def _format_passages(self, passages: List[ContentPassage], mode: str) -> tuple:
        """Create structured EVIDENCE list for entity-value extraction."""
        evidence_list = []
        provenance = []
        raw_texts = []

        for idx, passage in enumerate(passages, 1):
            meta = passage.metadata or {}
            title = meta.get('title') or meta.get('page_title') or ""
            provenance_id = meta.get('provenance', passage.source_url)
            snippet_date = meta.get('snippet_date')

            body_text = self._pick_body_text(passage, meta, mode)

            raw_texts.append(body_text)
            provenance.append(provenance_id)

            is_structured = self._looks_structured(body_text)

            # Keep body moderately bounded but preserve multi-line tables/lists
            # For structured content (tables), allow up to 5000 chars to avoid truncating large rosters
            if is_structured:
                snippet_body = body_text[:5000] if len(body_text) > 5000 else body_text
            else:
                snippet_body = body_text[:1000] if len(body_text) > 1000 else body_text
            evidence_item = {
                "id": idx,
                "url": provenance_id,
                "title": title,
                "snippet": snippet_body,
                "type": "snippet" if passage.source_url == "SERP_SNIPPET" else "webpage",
                "format": "structured" if is_structured else "plain"
            }
            if snippet_date:
                evidence_item["date"] = snippet_date

            evidence_list.append(evidence_item)

        # Format as JSON string for prompt
        evidence_json = json.dumps(evidence_list, ensure_ascii=False, indent=2)
        return evidence_json, provenance, raw_texts

    def _cache_key(self, task: PlanTask, passages: List[ContentPassage], stage_label: str) -> str:
        digest = hashlib.sha1()
        digest.update(task.variable_name.encode('utf-8'))
        digest.update(stage_label.encode('utf-8'))
        for passage in passages:
            base = passage.raw_text or passage.text or ""
            digest.update(self._summarize_for_cache(base).encode('utf-8', errors='ignore'))
        return digest.hexdigest()

    async def _extract_single_variable(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
        """Backward-compatible single variable extraction."""
        try:
            if self.client:
                result = await self._llm_extract(task, passages, stage_label="llm-deep")
                if result:
                    return result
            return self._fallback_extract(task, passages)
        except Exception as e:
            logger.error(f"Extraction failed for {task.variable_name}: {e}")
            return self._fallback_extract(task, passages)
    
    def _detect_language(self, text: str) -> str:
        """Detect if text is primarily Chinese or English"""
        if not text:
            return "en"
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text.strip())
        if total_chars == 0:
            return "en"
        return "zh" if chinese_chars / total_chars > 0.3 else "en"
    
    def _is_list_task(self, task: PlanTask) -> bool:
        """Check if task requires list/roster extraction"""
        var_name_lower = task.variable_name.lower()
        fact_lower = task.fact.lower()
        
        # Keywords that indicate list extraction
        list_keywords = [
            'list', 'roster', 'chiefs', 'heads', 'members', 'leaders', 
            'names', 'officials', 'ministers', 'secretaries', 'directors',
            'åå•', 'å±€é•¿', 'å¸é•¿', 'æˆå‘˜', 'é¢†å¯¼', 'å®˜å‘˜', 'éƒ¨é•¿'
        ]
        
        # Check variable name and fact
        for keyword in list_keywords:
            if keyword in var_name_lower or keyword in fact_lower:
                return True
        
        # Check category
        if task.category == "aggregation":
            # Additional check: fact asks for multiple entities
            if any(word in fact_lower for word in ['all', 'each', 'æ‰€æœ‰', 'å„', 'æ¯ä¸ª', 'who are']):
                return True
        
        return False
    
    def _direct_table_extract(self, task: PlanTask, passages: List[ContentPassage]) -> Optional[ExtractedVariable]:
        """
        Directly parse tables from passages for list/roster tasks.
        Bypass LLM to avoid uncertainty and conservative behavior.
        """
        all_entries = []
        provenance_urls = []
        
        for passage in passages:
            # Get raw text (which should contain [TABLE] markers)
            text = passage.raw_text or passage.text or ""
            
            if not text or '[TABLE]' not in text:
                continue
            
            # Extract table blocks
            table_blocks = re.findall(r'\[TABLE\](.*?)\[/TABLE\]', text, re.DOTALL)
            
            for table_text in table_blocks:
                entries = self._parse_table_block(table_text)
                if entries:
                    all_entries.extend(entries)
                    url = (passage.metadata or {}).get('provenance', passage.source_url)
                    if url not in provenance_urls:
                        provenance_urls.append(url)
        
        if not all_entries:
            print(f"[EXTRACTOR][DIRECT] No table entries found for {task.variable_name}")
            return None
        
        # Format entries as semicolon-separated list
        value = "; ".join(all_entries)
        
        # Calculate confidence based on number of entries
        if len(all_entries) >= 10:
            confidence = 0.95
        elif len(all_entries) >= 5:
            confidence = 0.90
        elif len(all_entries) >= 3:
            confidence = 0.85
        else:
            confidence = 0.75
        
        print(f"[EXTRACTOR][DIRECT] Extracted {len(all_entries)} entries from tables")
        
        return ExtractedVariable(
            variable_name=task.variable_name,
            value=value,
            confidence=confidence,
            provenance=provenance_urls,
            extraction_method="direct-table-parse",
            raw_passages=[text[:500] for text in [p.raw_text or p.text for p in passages if p.raw_text or p.text]]
        )
    
    def _parse_table_block(self, table_text: str) -> List[str]:
        """
        Parse a table block into list of entries.
        Expected format: "å±€å | å±€é•¿å§“å | å‰¯å±€é•¿ | æ”¿æ²»åŠ©ç†"
        Returns: ["å±€åï¼šå±€é•¿å§“åï¼ˆå¹´ä»½ï¼‰", ...] or ["å±€åï¼šå±€é•¿å§“å", ...] if no year
        
        Flexible extraction: year is optional, extract entries even without year info.
        """
        entries = []
        lines = table_text.strip().split('\n')
        
        if not lines:
            return entries
        
        # Skip title/caption lines
        data_lines = []
        for line in lines:
            line = line.strip()
            if not line or line.startswith('è¡¨æ ¼æ ‡é¢˜:') or line.startswith('---') or line == '------------':
                continue
            if '|' in line:
                data_lines.append(line)
        
        if not data_lines:
            return entries
        
        # First line might be header
        header_line = data_lines[0]
        headers = [h.strip() for h in header_line.split('|')]
        
        # Check if first line is header (contains generic terms)
        header_keywords = ['å§“å', 'name', 'èŒä½', 'position', 'title', 'å¹´ä»½', 'year', 'since', 'å±€', 'bureau', 'department', 'from', 'appointed']
        is_header = any(any(kw in h.lower() for kw in header_keywords) for h in headers)
        
        start_idx = 1 if is_header else 0
        
        # Parse data rows
        for line in data_lines[start_idx:]:
            cells = [c.strip() for c in line.split('|')]
            
            if len(cells) < 2:
                continue
            
            # First cell is usually position/bureau, second is name
            position = cells[0]
            name = cells[1]
            
            # Skip if name is "ä¸é©ç”¨", "å¾…å®š", empty, or generic placeholders
            skip_values = ['ä¸é©ç”¨', 'ä¸é€‚ç”¨', 'N/A', 'å¾…å®š', 'TBD', 'TBA', '-', '']
            if not name or name in skip_values:
                continue
            
            # Also skip if name looks like a section header (all Chinese chars but very generic)
            if name in ['å§“å', 'å±€é•¿', 'å¸é•¿', 'Name', 'Chief', 'Secretary']:
                continue
            
            # Try to find year/date in ALL cells (not just cells[2:])
            year = None
            date_info = None
            
            # Strategy 1: Look for 4-digit year (YYYY)
            for cell in cells:
                year_match = re.search(r'(20\d{2})', cell)
                if year_match:
                    year = year_match.group(1)
                    # Also try to find month
                    month_match = re.search(r'(20\d{2})[å¹´-](\d{1,2})', cell)
                    if month_match:
                        date_info = f"{month_match.group(1)}å¹´{month_match.group(2)}æœˆ"
                    else:
                        date_info = f"{year}å¹´"
                    break
            
            # Strategy 2: Look for Chinese date patterns (like "2022å¹´7æœˆ")
            if not date_info:
                for cell in cells:
                    cn_date_match = re.search(r'(20\d{2})å¹´(\d{1,2})æœˆ', cell)
                    if cn_date_match:
                        date_info = f"{cn_date_match.group(1)}å¹´{cn_date_match.group(2)}æœˆ"
                        break
            
            # Format entry based on available information
            # Priority: name + position (always) + date (if available)
            if date_info:
                entry = f"{position}ï¼š{name}ï¼ˆ{date_info}ï¼‰"
            elif year:
                entry = f"{position}ï¼š{name}ï¼ˆ{year}å¹´ï¼‰"
            else:
                # No year info available, still extract name + position
                entry = f"{position}ï¼š{name}"
            
            entries.append(entry)
        
        return entries
    
    def _extract_relevant_raw_material(self, raw_texts: List[str], task: PlanTask) -> str:
        """
        Extract relevant raw material from passages when LLM extraction fails.
        Intelligently selects and formats the most relevant parts.
        """
        if not raw_texts:
            return ""
        
        # Extract keywords from task for relevance matching
        task_keywords = set()
        
        # From variable name (split by underscore)
        var_parts = task.variable_name.lower().replace('_', ' ').split()
        task_keywords.update([p for p in var_parts if len(p) > 3])
        
        # From fact (extract meaningful words)
        fact_words = re.findall(r'\b[\u4e00-\u9fff]{2,}|\b[a-zA-Z]{4,}\b', task.fact.lower())
        task_keywords.update(fact_words[:10])  # Limit to avoid too many
        
        # Score each raw text by keyword relevance
        scored_texts = []
        for text in raw_texts:
            if not text or len(text.strip()) < 50:
                continue
            
            text_lower = text.lower()
            score = 0
            
            # Count keyword matches
            for keyword in task_keywords:
                if keyword in text_lower:
                    score += 1
            
            # Bonus for structured content (tables/lists)
            if '[TABLE]' in text:
                score += 5
            if re.search(r'^\s*[-â€¢*]\s', text, re.MULTILINE):
                score += 2
            
            scored_texts.append((score, text))
        
        if not scored_texts:
            # No scoring worked, just use first non-empty text
            for text in raw_texts:
                if text and len(text.strip()) > 50:
                    return self._format_raw_material(text[:1500])
            return ""
        
        # Sort by score (highest first) and take top passages
        scored_texts.sort(key=lambda x: x[0], reverse=True)
        
        # Combine top 2-3 passages
        selected = []
        total_chars = 0
        max_chars = 2000  # Limit total length
        
        for score, text in scored_texts[:3]:
            if score == 0:
                break  # No more relevant texts
            
            if total_chars + len(text) > max_chars:
                remaining = max_chars - total_chars
                if remaining > 200:  # Only add if there's meaningful space left
                    selected.append(text[:remaining] + "...")
                break
            
            selected.append(text)
            total_chars += len(text)
        
        if not selected:
            # Fallback: use first passage
            return self._format_raw_material(raw_texts[0][:1500])
        
        # Combine and format
        combined = "\n\n---\n\n".join(selected)
        return self._format_raw_material(combined)
    
    def _format_raw_material(self, text: str) -> str:
        """
        Format raw material for better readability.
        Add a prefix to indicate it's raw content.
        """
        if not text:
            return ""
        
        # Clean up excessive whitespace
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {2,}', ' ', text)
        
        # Add indicator prefix
        formatted = f"[åŸå§‹ææ–™]\n{text.strip()}"
        
        return formatted
    
    def _extract_core_entities(self, question: str) -> List[str]:
        """
        Extract core entities from the question for validation.
        Simple heuristic-based extraction (can be enhanced with NER later).
        """
        # Remove common question words
        question_words = ['what', 'when', 'where', 'who', 'why', 'how', 'which', 
                         'ä»€ä¹ˆ', 'ä½•æ—¶', 'å“ªé‡Œ', 'è°', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å“ªä¸ª', 'å¤šå°‘']
        
        # Tokenize and filter
        import jieba
        words = list(jieba.cut(question))
        
        # Filter out question words, punctuation, and short words
        entities = []
        for word in words:
            word_lower = word.lower().strip()
            if (len(word) >= 2 and 
                word_lower not in question_words and 
                not word.strip() in 'ï¼Œã€‚ï¼Ÿï¼ã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹' and
                not word.isdigit()):
                entities.append(word)
        
        # Return top entities (limit to avoid too many)
        return entities[:5]
    
    async def _llm_extract(
        self,
        task: PlanTask,
        passages: List[ContentPassage],
        stage_label: str = "llm",
        model_name: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: float = 0.1,
        mode: str = "passage",
        allow_fallback: bool = True,
        ctx: Optional[Any] = None
    ) -> Optional[ExtractedVariable]:
        """Use an LLM to extract information from provided passages."""

        if not passages:
            return None

        if not self.client:
            return self._fallback_extract(task, passages) if allow_fallback else None

        cfg = get_config()
        
        # ğŸ”¥ DIRECT TABLE PARSING for list/roster tasks (bypass LLM uncertainty)
        if self._is_list_task(task):
            direct_result = self._direct_table_extract(task, passages)
            if direct_result and direct_result.value and direct_result.confidence >= 0.7:
                print(f"[EXTRACTOR][DIRECT] Successfully extracted {task.variable_name} via table parsing (confidence={direct_result.confidence:.2f})")
                return direct_result
            else:
                print(f"[EXTRACTOR][DIRECT] Table parsing failed or low confidence, falling back to LLM")
        
        combined_text, provenance, raw_texts = self._format_passages(passages, mode)

        if not combined_text.strip():
            return None

        lang = self._detect_language(task.fact + " " + combined_text[:500])

        # Build structured extraction prompt
        plan_tasks_json = json.dumps([{
            "variable_name": task.variable_name,
            "fact": task.fact,
            "category": task.category
        }], ensure_ascii=False)

        # Extract time context
        time_rules = ""
        if ctx and hasattr(ctx, 'time_context') and ctx.time_context:
            tc = ctx.time_context
            if tc.window and tc.window[0]:
                target_date = tc.window[0].split('T')[0]
                if lang == "zh":
                    time_rules = f"\n\næ—¶é—´è§„åˆ™ï¼šç›®æ ‡æ—¥æœŸ = {target_date}ã€‚åªæ¥å—æ˜ç¡®æ ‡æ³¨æ­¤æ—¥æœŸçš„æ•°æ®ï¼Œå¿½ç•¥ã€Œä»Šå¤©ã€ã€ã€Œå½“å‰ã€ã€ã€Œå®æ—¶ã€æ ‡ç­¾ã€‚"
                else:
                    time_rules = f"\n\nTime Rule: Target date = {target_date}. Only accept data explicitly marked with this date. Ignore 'today', 'current', 'real-time' labels."

        if lang == "zh":
            system_msg = """ä½ æ˜¯â€œç»“æ„åŒ–æå–ä¸“å®¶ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰â€ã€‚åªä» EVIDENCE ä¸­æŠ½å– PLAN_TASKS éœ€è¦çš„äº‹å®ï¼›ç»ä¸å¤–æ¨æˆ–å¼•å…¥å¸¸è¯†ã€‚ä¸¥æ ¼å¯¹é½ã€å°±è¿‘å–è¯ã€æ—¶é—´å¯éªŒè¯ã€‚

ã€èŒƒå›´/è¯é¢˜æ§åˆ¶ã€‘
- ä»…å¤„ç† PLAN_TASKS ä¸­çš„å˜é‡ï¼›ä¸ä»»åŠ¡æ— å…³çš„äº‹å®ä¸€æ¦‚å¿½ç•¥ã€‚
- è‹¥ EVIDENCE å«å¤šå®ä½“ï¼Œä»…åœ¨"ç›®æ ‡å®ä½“åŠå…¶åˆ«å"å°±è¿‘èŒƒå›´å†…å–å€¼ã€‚

ã€æå–æ¨¡å¼é€‰æ‹© - ä¼˜å…ˆçº§è§„åˆ™ã€‘
**é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸º"åˆ—è¡¨æå–æ¨¡å¼"ï¼ˆè§ä¸‹æ–¹ä¸“é—¨è§„åˆ™ï¼‰ã€‚å¦‚æœæ˜¯ï¼Œè·³è¿‡"ä¸‰é”šè§„åˆ™"ï¼Œç›´æ¥ä½¿ç”¨è¡¨æ ¼/åˆ—è¡¨æå–é€»è¾‘ã€‚**
**åªæœ‰éåˆ—è¡¨ç±»ä»»åŠ¡æ‰éœ€è¦ä¸¥æ ¼éµå¾ª"ä¸‰é”šè§„åˆ™"ã€‚**

ã€å®ä½“ä¸æ—¶é—´çš„"ä¸‰é”š"è§„åˆ™ã€‘ï¼ˆä»…é€‚ç”¨äºéåˆ—è¡¨ç±»ä»»åŠ¡ï¼‰
æŠ½å–ä»»ä½•"å¸¦æ—¶é—´çš„äº‹å®"ï¼ˆäº‹ä»¶/å¥–é¡¹/ä»»å…/ä»»æœŸç­‰ï¼‰æ—¶ï¼Œå¿…é¡»åœ¨åŒä¸€æ¥æºæ–‡æœ¬çš„"åŒå¥/åŒè¡Œæˆ–Â±120å­—ç¬¦"å†…ï¼ŒåŒæ—¶å‡ºç°ï¼š
1) ç›®æ ‡å®ä½“çš„æ˜ç¡®æåŠï¼ˆå§“å/æœºæ„/åˆ«åä¹‹ä¸€ï¼‰ï¼›
2) äº‹ä»¶è§¦å‘è¯ï¼ˆè§ä¸‹ï¼‰æˆ–ç­‰ä»·çŸ­è¯­ï¼›
3) å¯è§„çº¦çš„æ—¥æœŸè¡¨è¾¾ï¼ˆYYYY[-MM[-DD]] æˆ–â€œYYYYâ€“YYYYâ€åŒºé—´ç­‰ï¼‰ã€‚
ä¸‰è€…ä»»ä¸€ç¼ºå¤± â†’ è¯¥å€™é€‰æ ‡æ³¨ `proximity_ok:false`ï¼›è‹¥æ— ä»»ä½• `proximity_ok:true` çš„å€™é€‰ â†’ `status:"unverified"`ã€‚

ã€äº‹ä»¶è§¦å‘è¯è¯è¡¨ï¼ˆéƒ¨åˆ†ï¼‰ã€‘
- å½“é€‰/å½“é¸/elected/reelectedã€ä»»å‘½/appointedã€å°±ä»»/assumed office/sworn in/inauguratedã€ä»£ç†/actingã€ç»­ä»»/è¿ä»»/reappointedã€å¸ä»»/stepped down/retired/removedã€ä»»æœŸå¼€å§‹/ç»“æŸ/term start/endã€è·æˆ/æˆäºˆ/è¢«è¯„ä¸º/awarded/conferredã€å½“é€‰é™¢å£«/fellow ofâ€¦
- å¯¹å¤´è¡”/èŒä½å˜åŠ¨ï¼ŒåŒºåˆ†ï¼šè·ä»»å‘½ vs å°±èŒï¼ˆå®é™…å¼€å§‹ï¼‰ vs å¸ä»»ï¼›ä¸å¯æ··ç”¨ã€‚

ã€æ—¥æœŸä¸ä»»æœŸè§„çº¦ã€‘
- ä»…æ¥å—ä¸å®ä½“+äº‹ä»¶è§¦å‘è¯ç›¸é‚»è¿‘çš„æ—¥æœŸï¼›å¿½ç•¥é¡µé¢å…ƒæ•°æ®ï¼ˆâ€œæ›´æ–°äº/å‘å¸ƒäº/æœ€åä¿®æ”¹â€ï¼‰å’Œâ€œas of/æˆªè‡³/ç›®å‰/ç°ä»»â€ç±»â€œçŠ¶æ€æ—¶é—´â€ã€‚
- è§£æèŒƒå›´ï¼šç²¾ç¡®æ—¥/æœˆ/å¹´ã€å­£åº¦ï¼ˆQ1â€“Q4â†’ä»¥å­£åº¦é¦–æœˆè§„çº¦ï¼‰ã€ä¸­è‹±æ–‡æ—¥æœŸã€ä¸­æ–‡æ•°å­—æ—¥æœŸã€ç ´æŠ˜å·åŒºé—´ï¼ˆå¦‚ 2015â€“2018ã€2015å¹´1æœˆâ€“2017å¹´3æœˆï¼‰ã€‚
- â€œè‡³ä»Š/åœ¨ä»»/ç°ä»»/presentâ€â†’ ä»…åœ¨æ˜ç¡®å­˜åœ¨â€œä»»æœŸå¼€å§‹â€ä¸”å®ä½“åŒ¹é…æ—¶ï¼Œè¾“å‡º open-ended ç»“æŸï¼Œå€¼ç”¨å­—ç¬¦ä¸²åŒºé—´ï¼ˆå¦‚ "2019-03â€“present"ï¼‰ï¼Œprecision å–å¼€å§‹ç«¯ç²¾åº¦ï¼›å¦åˆ™ `unverified`ã€‚
- ä¸ä»â€œè·å¥–å¹´ä»½ Xâ€æ¨æ–­â€œä»»æœŸå¼€å§‹/æ¯•ä¸šå¹´ä»½â€ç­‰ä»»ä½•æœªæ˜ç¤ºçš„æ—¶é—´ã€‚

ã€ğŸ”¥ åˆ—è¡¨æå–æ¨¡å¼ - ä¼˜å…ˆè§„åˆ™ï¼ˆè¦†ç›–ä¸‰é”šè§„åˆ™ï¼‰ğŸ”¥ã€‘
**è§¦å‘æ¡ä»¶**ï¼švariable_name åŒ…å« "list/roster/chiefs/heads/members/leaders/names"ï¼Œæˆ– category="aggregation" ä¸”è¦æ±‚å¤šä¸ªå®ä½“

**ä¸€æ—¦è§¦å‘åˆ—è¡¨æå–æ¨¡å¼ï¼Œå®Œå…¨å¿½ç•¥"ä¸‰é”šè§„åˆ™"å’Œ"äº‹ä»¶è§¦å‘è¯"è¦æ±‚ï¼Œæ”¹ç”¨ä»¥ä¸‹ç®€åŒ–è§„åˆ™ï¼š**

1. **[TABLE]...[/TABLE] è¡¨æ ¼æå–**ï¼š
   - è¯†åˆ«åˆ—å¤´ï¼šå§“å/Nameã€èŒä½/Position/Titleã€å¹´ä»½/Year/Since/From/Appointed
   - æå–è¡¨æ ¼ä¸­**æ‰€æœ‰è¡Œ**çš„æ•°æ®ï¼š
     * **å¿…éœ€**ï¼šå§“å + èŒä½ï¼ˆåŒä¸€è¡Œå†…ï¼‰
     * **å¯é€‰**ï¼šå¹´ä»½ï¼ˆå¦‚æœè¡¨æ ¼åˆ—æœ‰å¹´ä»½å°±æå–ï¼Œæ²¡æœ‰å°±ä¸æå–ï¼‰
     * âš ï¸ **å³ä½¿ä»»åŠ¡åæ˜¯ "with_years"ï¼Œä¹Ÿè¦æå–æ²¡æœ‰å¹´ä»½çš„æ¡ç›®**
   - âš ï¸ ä¸éœ€è¦"äº‹ä»¶è§¦å‘è¯"ï¼ˆè¡¨æ ¼æœ¬èº«å°±æ˜¯äº‹å®é™ˆè¿°ï¼‰
   - âš ï¸ ä¸éœ€è¦éªŒè¯"Â±120å­—ç¬¦"è§„åˆ™
   - æ ¼å¼åŒ–è¾“å‡ºï¼š
     * æœ‰å¹´ä»½ï¼š"èŒä½1ï¼šå§“å1ï¼ˆ2022å¹´7æœˆï¼‰; èŒä½2ï¼šå§“å2ï¼ˆ2020å¹´ï¼‰; ..."
     * æ— å¹´ä»½ï¼š"èŒä½1ï¼šå§“å1; èŒä½2ï¼šå§“å2; ..."
     * æ··åˆæƒ…å†µï¼š"èŒä½1ï¼šå§“å1ï¼ˆ2022å¹´ï¼‰; èŒä½2ï¼šå§“å2; èŒä½3ï¼šå§“å3ï¼ˆ2023å¹´ï¼‰; ..."

2. **æ–‡æœ¬åˆ—è¡¨æå–**ï¼ˆå¦‚"å±€é•¿ï¼šXXXï¼ˆ2022å¹´ï¼‰"ï¼‰ï¼š
   - è¯†åˆ«æ¨¡å¼ï¼š"èŒä½[ï¼š:] å§“å [å¯é€‰ï¼š(å¹´ä»½)]"
   - æå–æ‰€æœ‰åŒ¹é…çš„åå­—

3. **è¾“å‡ºè¦æ±‚**ï¼š
   - âœ… æ­£ç¡®ç¤ºä¾‹ï¼š"å…¬å‹™å“¡äº‹å‹™å±€é•·ï¼šæ¥Šä½•è““èŒµ; æ”¿åˆ¶åŠå…§åœ°äº‹å‹™å±€é•·ï¼šæ›¾åœ‹è¡ï¼ˆ2022å¹´7æœˆï¼‰; æ–‡åŒ–é«”è‚²åŠæ—…éŠå±€é•·ï¼šç¾…æ·‘ä½©; ..."
   - âŒ é”™è¯¯ç¤ºä¾‹ï¼š"æœ‰15ä¸ªå±€é•¿"ï¼ˆç¦æ­¢æ€»ç»“ï¼Œå¿…é¡»åˆ—å‡ºæ‰€æœ‰åå­—ï¼‰
   - âŒ é”™è¯¯ç¤ºä¾‹ï¼š"åŒ…æ‹¬å¤šä¸ªå†³ç­–å±€"ï¼ˆå¿…é¡»ç»™å‡ºå…·ä½“åå•ï¼‰

4. **Confidence è¯„åˆ†**ï¼ˆæ”¾å®½æ ‡å‡†ï¼‰ï¼š
   - æ‰¾åˆ° â‰¥3 ä¸ªå®ä½“ â†’ status:"ok", confidence=0.9
   - æ‰¾åˆ° 1-2 ä¸ª â†’ status:"ok", confidence=0.7
   - ä¸€ä¸ªéƒ½æ²¡æ‰¾åˆ° â†’ status:"unverified", confidence=0.6

**å…³é”®**ï¼šåˆ—è¡¨æ¨¡å¼ä¸‹ï¼Œå³ä½¿ç¼ºå°‘å¹´ä»½æˆ–äº‹ä»¶è§¦å‘è¯ï¼Œåªè¦æœ‰å§“å+èŒä½ï¼Œå°±åº”è¯¥æå–å¹¶è¿”å› status:"ok"ã€‚

ã€å†²çªä¸å»æ­§ä¹‰ã€‘
- åŒä¸€å˜é‡è‹¥ä¸åŒæ¥æº/åŒæ¥æºä¸åŒç‰‡æ®µç»™å‡º**ä¸ç›¸åŒå€¼**ä¸”å‡æ»¡è¶³â€œä¸‰é”šâ€â†’ `status:"conflict"`ï¼›åœ¨ candidates ä¸­åˆ—å‡ºå…¨éƒ¨ï¼Œæ³¨æ˜è·ç¦»ä¸è§¦å‘è¯ã€‚
- è‹¥å€¼ç›¸åŒä½†ç²¾åº¦ä¸åŒï¼Œé€‰æ‹©ç²¾åº¦æ›´é«˜è€…ä¸ºä¸»å€¼ï¼›ä¿ç•™å¦ä¸€ä¸ªä¸ºå€™é€‰ã€‚
- æºä¼˜å…ˆçº§ï¼ˆä»…ä½œå¹¶åˆ—æ—¶çš„æœ€åå†³ç­–å› å­ï¼‰ï¼šå®˜æ–¹/ä¸»åŠæ–¹/æ³•è§„ > å­¦æ ¡/ç ”ç©¶æ‰€/åä¼š > ä¸€çº¿åª’ä½“/æ•°æ®åº“ > èšåˆå™¨/ç™¾ç§‘/ä¸ªäººåšå®¢ã€‚

ã€è¾“å‡ºè´¨é‡é—¨æ§›ï¼ˆä»»ä¸€ä¸æ»¡è¶³åˆ™é™çº§ä¸º unverifiedï¼‰ã€‘
- ç¼ºå°‘æ˜ç¡®å®ä½“é”šç‚¹ï¼›æˆ–å®ä½“é”šç‚¹ä¸æ—¥æœŸ/äº‹ä»¶ä¸åœ¨åŒåŸŸè¿‘é‚»ï¼ˆÂ±120å­—ç¬¦ï¼‰ï¼›
- ä»…å‡ºç°â€œè¢«æå/æ‹Ÿä»»/ä¼ é—»/é¢„æµ‹/å¾…å®¡â€ï¼›
- ä»…å‡ºç°â€œåœ¨ä»»/æˆªè‡³æŸæ—¥ä»ä»»â€å´æ— å¼€å§‹æ—¶é—´ä¸” PLAN_TASKS è¦æ±‚å¼€å§‹/ç»“æŸï¼›
- ä»»æœŸåŒºé—´å¼€å§‹>ç»“æŸæˆ–é‡å /å€’ç½®ï¼›
- é™¢å£«/fellow æœªç»™å‡ºå®Œæ•´æˆäºˆæœºæ„åã€‚

ã€é›†åˆ/åŸºæ•°æ§åˆ¶ã€‘
- PLAN_TASKS å¯ä¸ºæ¯ä¸ªå˜é‡æ ‡è®° cardinality: "SINGLE" | "LIST"ï¼ˆå¦‚ç¼ºçœï¼ŒæŒ‰ SINGLE å¤„ç†ï¼‰ã€‚
- å½“ cardinality="LIST"ï¼šè¿”å›æŒ‰ items[] ç»„ç»‡çš„é›†åˆï¼›æ¯ä¸ª item éƒ½å¿…é¡»æ»¡è¶³â€œä¸‰é”šè¿‘é‚»â€ï¼ˆå®ä½“+äº‹ä»¶è§¦å‘è¯+æ—¥æœŸï¼‰æ‰è®¡å…¥ã€‚è‹¥æ— ä»»ä½•æ»¡è¶³é¡¹ â†’ status:"unverified"ã€‚
- å¯å°Šé‡ PLAN_TASKS çš„é™åˆ¶ï¼šmax_itemsã€year_rangeã€type_filterï¼ˆå¦‚ {"type":["award","education"]}ï¼‰ï¼Œç”¨äºæ§åˆ¶æ•°é‡ä¸èŒƒå›´ã€‚

ã€å»é‡/å½’å¹¶ä¸å†²çªã€‘
- å¯¹ LISTï¼šå…ˆå¯¹å€™é€‰æŒ‰ï¼ˆè§„èŒƒåŒ–åç§° + ç»„ç»‡/é™¢ç³» + ä¸»æ—¥æœŸ/èµ·æ­¢åŒºé—´ï¼‰èšç±»å»é‡ï¼›åŒç°‡å†…æ—¥æœŸä¸€è‡´ä¸”ç²¾åº¦æ›´é«˜è€…ä¸ºä¸»å€¼ï¼Œå…¶ä½™å…¥ candidatesã€‚
- åŒç°‡å†…è‹¥å‡ºç°**ä¸åŒå€¼ä¸”å‡æ»¡è¶³ä¸‰é”š** â†’ è¯¥ç°‡æ ‡æ³¨ conflict:trueï¼Œå¹¶åœ¨ notes ä¸ candidates ä¸­å±•ç¤ºï¼›è‹¥å­˜åœ¨ â‰¥1 ä¸ªå†²çªç°‡ â†’ æœ¬å˜é‡ status:"conflict"ï¼›å¦åˆ™è‹¥ items éç©º â†’ status:"ok"ï¼›å¦åˆ™ "unverified"ã€‚

ã€å€™é€‰è®°å½•è¦æ±‚ï¼ˆç”¨äºå®¡æ ¸ï¼‰ã€‘
- æ¯ä¸ªå€™é€‰é¡»ç»™å‡ºï¼švalueã€precisionã€source_idã€urlã€proximity_okã€è§¦å‘è¯ï¼ˆevent_triggerï¼‰ã€å®ä½“å‘½ä¸­ï¼ˆentity_mentionï¼‰ã€æ—¥æœŸåŸæ–‡ï¼ˆdate_mentionï¼‰ã€è¿‘é‚»è·ç¦»ï¼ˆdistance_charsï¼‰ã€å¼•ç”¨ç‰‡æ®µï¼ˆquote â‰¤ 200 å­—ï¼‰ã€‚

ä¸¥æ ¼éµå¾ªä»¥ä¸‹ I/O æ¨¡æ¿ã€‚é™¤ JSON å¤–ä¸å¾—è¾“å‡ºä»»ä½•æ–‡å­—ã€‚
"""
            prompt = f"""INPUT
PLAN_TASKS: {plan_tasks_json}
EVIDENCE: {combined_text}{time_rules}

OUTPUT JSON SCHEMA
{{
  "results": [
    {{
      "variable_name": "{task.variable_name}",
      "status": "ok" | "unverified" | "conflict",
      "value": "string|number|null",
      "precision": "day"|"month"|"year"|null,
      "source_ids": [1,2],
      "candidates": [
        {{
          "value":"...",
          "precision":"day|month|year|null",
          "source_id":1,
          "url":"...",
          "proximity_ok": true,
          "event_trigger":"appointed|elected|term start|award|...",
          "entity_mention":"åŸæ–‡ä¸­çš„å®ä½“æåŠ",
          "date_mention":"åŸæ–‡ä¸­çš„æ—¥æœŸè¡¨è¾¾",
          "distance_chars": 87,
          "quote":"ï¼ˆå«å®ä½“/è§¦å‘è¯/æ—¥æœŸçš„åŸæ–‡ç‰‡æ®µ â‰¤200å­—ï¼‰"
        }}
      ],
      "notes": "ç®€çŸ­åŸå› ï¼ˆå¦‚ï¼šå®ä½“/äº‹ä»¶/æ—¥æœŸä¸‰é”šé½å¤‡ï¼›æˆ–ä¸ºä½• unverified/conflictï¼‰"
    }}
  ]
}}

ä»…è¿”å›JSONï¼Œä¸è¦è§£é‡Šã€‚"""
        else:
            system_msg = """You are a "Structured Extractor â€“ Strict Mode". Extract ONLY the facts required by PLAN_TASKS from EVIDENCE. No world knowledge, no guessing.

[Scope & Topic Control]
- Only handle variables listed in PLAN_TASKS; ignore everything else.
- If multiple entities appear, extract values ONLY within the local neighborhood of the target entity or its aliases.

[Extraction Mode Selection - Priority Rule]
**First check if this is "List Extraction Mode" (see dedicated rules below). If yes, SKIP the "Triple-Anchor Rule" and use table/list extraction logic directly.**
**Only non-list tasks need to strictly follow the "Triple-Anchor Rule".**

[Entityâ€“Eventâ€“Date "Triple-Anchor" Rule] (only for non-list tasks)
For any time-stamped fact (events/awards/appointments/tenure), in the SAME source text and within the SAME sentence/line or Â±120 characters, you must find:
1) an explicit mention of the target entity (name/alias/org),
2) an event trigger (see list below) or equivalent phrase,
3) a normalizable date expression (YYYY[-MM[-DD]] or a clear range).
If any anchor is missing â†’ mark that candidate `proximity_ok:false`. If NO candidate has `proximity_ok:true` â†’ `status:"unverified"`.

[Event Trigger Lexicon (partial)]
elected/reelected, appointed, assumed office/sworn in/inaugurated, acting, reappointed, stepped down/retired/removed, term start/term end, awarded/conferred, fellow of â€¦

[Date & Tenure Normalization]
- Accept dates ONLY near the entity + event trigger; ignore page meta dates (â€œupdated on/published on/last modifiedâ€) and status-time phrases (â€œas of/currentlyâ€).
- Parse day/month/year, quarters (Q1â€“Q4â†’map to first month), EN/CN dates, ranges (e.g., 2015â€“2018, Jan 2015â€“Mar 2017).
- â€œpresent/currentâ€ is allowed only if a clear term start exists for the entity; encode as "YYYY[-MM[-DD]â€“present" with precision from the start; otherwise `unverified`.
- Do NOT infer missing dates (e.g., do not derive start year from award year).

[ğŸ”¥ List Extraction Mode - Priority Rule (Overrides Triple-Anchor) ğŸ”¥]
**Trigger**: variable_name contains "list/roster/chiefs/heads/members/leaders/names", OR category="aggregation" requiring multiple entities

**Once List Extraction Mode is triggered, COMPLETELY IGNORE "Triple-Anchor Rule" and "event trigger" requirements. Use these simplified rules instead:**

1. **[TABLE]...[/TABLE] Table Extraction**:
   - Identify headers: Name, Position/Title, Year/Since/From/Appointed
   - Extract **ALL rows** from table:
     * **Required**: name + position (in same row)
     * **Optional**: year (extract if column exists, skip if not available)
     * âš ï¸ **Even if task name is "with_years", extract entries WITHOUT year too**
   - âš ï¸ NO "event trigger" required (table itself is factual statement)
   - âš ï¸ NO "Â±120 chars" proximity validation needed
   - Format output:
     * With year: "Position1: Name1 (July 2022); Position2: Name2 (2020); ..."
     * Without year: "Position1: Name1; Position2: Name2; ..."
     * Mixed: "Position1: Name1 (2022); Position2: Name2; Position3: Name3 (2023); ..."

2. **Text List Extraction** (e.g., "Chief: John Doe (2022)"):
   - Recognize pattern: "position[: ] name [optional: (year)]"
   - Extract all matching names

3. **Output Requirements**:
   - âœ… GOOD: "Secretary for the Civil Service: Ada Chung; Secretary for Constitutional and Mainland Affairs: Erick Tsang (July 2022); ..."
   - âŒ BAD: "There are 15 bureau chiefs" (no summarizing, must list all names)
   - âŒ BAD: "Multiple policy bureaus" (must provide specific roster)

4. **Confidence Scoring** (relaxed standards):
   - Found â‰¥3 entities â†’ status:"ok", confidence=0.9
   - Found 1-2 entities â†’ status:"ok", confidence=0.7
   - Found none â†’ status:"unverified", confidence=0.6

**Critical**: In list mode, even without year or event trigger, if name+position exists, extract it and return status:"ok".

[Conflicts & Disambiguation]
- If TWO different values both satisfy the triple-anchor rule â†’ `status:"conflict"` and list all in candidates with distance and trigger.
- If values match but precisions differ â†’ choose the higher precision as main value; keep the other as candidate.
- Source priority (tie-breaker only): official/organizer/law > university/association > tier-1 media/databases > aggregators/wiki/blogs.

[Quality Gates â†’ downgrade to unverified if ANY holds]
- Missing entity anchor; OR entity/date not near the event trigger (Â±120 chars);
- Nomination/rumor/prediction/pending only;
- â€œas of/currentlyâ€ without a clear start date when start/end is required;
- Term ranges reversed/overlapping;
- â€œfellow/academicianâ€ without a full granting organization name.

[Candidate Record (for audit)]
Each candidate must include: value, precision, source_id, url, proximity_ok, event_trigger, entity_mention, date_mention, distance_chars, and a short quote (â‰¤200 chars) containing the three anchors.

Return STRICT JSON only. No extra text.
"""
            prompt = f"""INPUT
PLAN_TASKS: {plan_tasks_json}
EVIDENCE: {combined_text}{time_rules}

OUTPUT JSON SCHEMA
{{
  "results": [
    {{
      "variable_name": "{task.variable_name}",
      "status": "ok" | "unverified" | "conflict",
      "value": "string|number|null",
      "precision": "day"|"month"|"year"|null,
      "source_ids": [1,2],
      "candidates": [
        {{
          "value":"...",
          "precision":"day|month|year|null",
          "source_id":1,
          "url":"...",
          "proximity_ok": true,
          "event_trigger":"appointed|elected|term start|award|...",
          "entity_mention":"exact mention in text",
          "date_mention":"raw date string",
          "distance_chars": 87,
          "quote":"(â‰¤200 chars snippet containing entity+trigger+date)"
        }}
      ],
      "notes": "short reason (e.g., triple-anchor satisfied; or why unverified/conflict)"
    }}
  ]
}}

Return JSON only, no explanation."""

        model_name = model_name or self.model_name
        max_tokens = max_tokens or cfg.get('models.extractor.max_tokens', 4000)

        try:
            response = self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}
            )
        except Exception as api_error:
            print(f"[EXTRACTOR][WARN] response_format not supported ({api_error}), retrying without it")
            response = self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )

        message = response.choices[0].message
        result_text = (message.content or "").strip()
        if not result_text:
            print(f"[EXTRACTOR][ERROR] Empty LLM response for {task.variable_name}")
            return self._fallback_extract(task, passages) if allow_fallback else None

        try:
            result_data = json.loads(result_text)
        except json.JSONDecodeError as err:
            print(f"[EXTRACTOR][ERROR] JSON decode failed for {task.variable_name}: {err}")
            print(f"[EXTRACTOR][DEBUG] Raw response: {result_text[:200]}")
            return self._fallback_extract(task, passages) if allow_fallback else None

        # Parse new structured format
        results = result_data.get("results", [])
        if not results:
            print(f"[EXTRACTOR][WARN] No results in structured response for {task.variable_name}")
            return self._fallback_extract(task, passages) if allow_fallback else None
        
        result = results[0]  # Get first result (single task extraction)
        status = result.get("status", "ok")
        value = result.get("value")
        notes = result.get("notes", "")
        
        # Calculate confidence based on status and candidates
        if status == "ok":
            confidence = 0.95
        elif status == "unverified":
            confidence = 0.60
        elif status == "conflict":
            confidence = 0.40
        else:
            confidence = 0.50
        
        # If multiple candidates, reduce confidence slightly
        candidates = result.get("candidates", [])
        if len(candidates) > 1:
            confidence *= 0.9
        
        # ğŸ”¥ FALLBACK: If extraction failed (None or low confidence), use raw material
        if (not value or confidence < 0.7) and raw_texts:
            print(f"[EXTRACTOR][FALLBACK] Extraction failed for {task.variable_name}, using raw material")
            # Combine relevant raw texts (limited to avoid huge responses)
            raw_material = self._extract_relevant_raw_material(raw_texts, task)
            if raw_material and len(raw_material) > 50:  # At least some meaningful content
                value = raw_material
                confidence = 0.65  # Low but not too low
                status = "raw_fallback"
                notes = f"LLMæå–å¤±è´¥ï¼Œè¿”å›åŸå§‹ææ–™æ®µè½ã€‚åŸå› ï¼š{notes}"
                print(f"[EXTRACTOR][FALLBACK] Using {len(raw_material)} chars of raw material")
        
        # Map source_ids to provenance URLs
        source_ids = result.get("source_ids", [])
        selected_provenance = [provenance[i-1] for i in source_ids if 1 <= i <= len(provenance)]
        if not selected_provenance:
            selected_provenance = provenance  # Fallback to all

        if cfg.is_decision_logging_enabled('ir_rag'):
            print(
                f"[EXTRACTOR][DEBUG] {stage_label} {task.variable_name}: value={str(value)[:50] if value else 'None'}... "
                f"confidence={confidence:.2f} status={status} notes={notes[:50]}..."
            )

        return ExtractedVariable(
            variable_name=task.variable_name,
            value=value,
            confidence=confidence,
            provenance=selected_provenance,
            extraction_method=f"{stage_label}-structured" if status != "raw_fallback" else "raw-fallback",
            raw_passages=raw_texts
        )


    def _fallback_extract(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
        """Fallback extraction using simple heuristics"""
        if not passages:
            return ExtractedVariable(
                variable_name=task.variable_name,
                value=None,
                confidence=0.0,
                extraction_method="fallback"
            )

        # Use the highest-scored passage as the answer
        best_passage = passages[0]

        # Simple extraction based on task category
        if task.category == "biography":
            value = self._extract_biographical_info(best_passage.text)
        elif task.category == "fact_verification":
            value = self._extract_factual_claim(best_passage.text)
        else:
            # Generic extraction - use first sentence or paragraph
            sentences = best_passage.text.split('.')
            value = sentences[0].strip() if sentences else best_passage.text[:200]

        return ExtractedVariable(
            variable_name=task.variable_name,
            value=value,
            confidence=0.6,
            provenance=[(p.metadata or {}).get('provenance', p.source_url) for p in passages],
            extraction_method="fallback",
            raw_passages=[(p.raw_text or p.text) for p in passages]
        )
    
    def _extract_biographical_info(self, text: str) -> str:
        """Extract biographical information"""
        # Look for patterns like "born in", "graduated from", etc.
        bio_patterns = [
            r'born (?:in|on) ([^.]+)',
            r'graduated from ([^.]+)',
            r'worked at ([^.]+)',
            r'known for ([^.]+)'
        ]
        
        for pattern in bio_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback to first sentence
        sentences = text.split('.')
        return sentences[0].strip() if sentences else text[:200]
    
    def _extract_factual_claim(self, text: str) -> str:
        """Extract factual claims"""
        # Look for definitive statements
        fact_patterns = [
            r'(was founded in \d{4})',
            r'(established in \d{4})',
            r'(created in \d{4})',
            r'(\d{4}[^.]*founded)'
        ]
        
        for pattern in fact_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback to first sentence
        sentences = text.split('.')
        return sentences[0].strip() if sentences else text[:200]



# ============================================================================
# Test/Demo Code
# ============================================================================

if __name__ == "__main__":
    print("[TEST][Extractor] InformationExtractor module loaded successfully")
    print(f"[TEST][Extractor] Module contains {len(InformationExtractor.__dict__)} methods")
    print("=" * 80)
