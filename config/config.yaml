# WebChasor Configuration File
# This file contains all configuration settings for the WebChasor system
# Each section controls different aspects of the system's behavior

# Demo and output configuration
demo:
  streaming:
    enabled: false  # Enable/disable streaming output (OpenAI SSE format)
    format: "openai"  # Output format: "openai" for SSE, "simple" for plain text

# External service configurations
external_services:
  openai:
    api_base: "https://oneapi.hkgai.net/v1"  # OpenAI API endpoint
    rate_limit: 3500  # Requests per minute for OpenAI API
    retry_delay: 1    # Delay in seconds between retries
  serpapi:
    rate_limit: 100   # Requests per minute for SerpAPI
    retry_delay: 1    # Delay in seconds between retries
  google:
    cse_id: "545488a3237a4d40"  # Google Custom Search Engine ID
  vector_db:
    connection_pool_size: 10  # Maximum number of database connections

# Information Retrieval and RAG (Retrieval-Augmented Generation) settings
ir_rag:
  # Content processing configuration
  content:
    chunk_overlap: 50           # Overlap between text chunks in characters
    chunk_size: 500             # Size of text chunks in characters
    max_passages_per_task: 6   # Maximum passages to retrieve per task (balanced for speed and coverage)
    min_passage_length: 20      # Minimum length for valid passages
  
  # Information extraction settings
  extraction:
    confidence_threshold: 0.6      # Minimum confidence score for extractions
    enable_ner_fallback: true     # Enable Named Entity Recognition fallback
    enable_regex_fallback: true   # Enable regex pattern fallback
    max_extraction_attempts: 3    # Maximum attempts for extraction
  
  # Content ranking algorithm settings
  ranking:
    algorithm: keyword_matching           # Ranking algorithm type
    entity_weight: 3.5                   # Weight for entity matches (increased to 3.5 for stronger entity prioritization)
    keyword_weight: 1.0                  # Weight for keyword matches
    length_penalty_threshold: 20         # Threshold for length penalty
    question_word_weight: 0.5           # Weight for question words
    structure_bonus: 0.2                # Bonus for structured content
  
  # Search engine configuration
  search:
    engine: google           # Search engine to use
    language: zh-cn         # Search language (Chinese Traditional Hong Kong - most accurate for HK)
    location: Hong Kong     # Geographic location for search
    gl: hk                  # Geo-location country code (prioritizes .hk domains)
    max_results: 10        # Maximum search results to return
    # Single provider: 'serpapi', 'google_custom_search', or 'gcp_vertex_search'
    # Multiple providers (comma-separated): 'serpapi, gcp_vertex_search' (merges results from all)
    provider: serpapi, gcp_vertex_search
    safe_search: true      # Enable safe search filtering
    # Time-based search filter (tbs parameter)
    # Options: null (no filter), 'qdr:h' (past hour), 'qdr:d' (past day), 
    #          'qdr:w' (past week), 'qdr:m' (past month), 'qdr:y' (past year)
    time_filter: null     # Temporarily disabled due to SerpAPI compatibility
    # Batch search concurrency settings (for multiple queries)
    concurrent: 2          # Max concurrent searches in batch mode (reduced for Google API limits)
    qps: 1                 # Queries per second rate limit (reduced for Google API limits)
    retries: 2             # Retry attempts per query
  
  # Vector database settings
  vector_db:
    enabled: false                    # Enable vector database functionality
    hybrid_weight: 0.5               # Weight for hybrid search
    provider: faiss                  # Vector database provider
    similarity_threshold: 0.8        # Minimum similarity threshold
    top_k: 10                        # Number of top results to return
  
  # Web scraping configuration
  web_scraping:
    enabled: true                    # Enable web scraping functionality
    max_pages: 3                   # Maximum pages to scrape per request
    parallel_fetch: true            # Enable parallel page fetching
    retry_attempts: 2               # Number of retry attempts
    timeout: 10                     # Timeout in seconds
    user_agent: Mozilla/5.0 (compatible; WebChasor/1.0)  # User agent string

# Logging configuration
logging:
  # Decision logging settings for different components
  decisions:
    enabled: true        # Enable decision logging globally
    ir_rag: true        # Log IR_RAG decisions
    productivity: true  # Log productivity decisions
    reasoning: true     # Log reasoning decisions
    registry: true      # Log registry decisions
    router: true        # Log router decisions
    synthesizer: true   # Log synthesizer decisions
  
  # File logging settings
  file:
    backup_count: 5              # Number of backup log files
    enabled: false               # Enable file logging
    max_size: 10MB              # Maximum log file size
    path: logs/webchasor.log    # Log file path
  
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'  # Log format
  level: INFO  # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

# AI Model configurations for different components
models:
  # Information extraction model
  extractor:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 2000            # Maximum tokens for extraction
    model_name: gpt-oss-20b   # Model name for extraction
    provider: openai            # Model provider
    temperature: 0.1            # Temperature for deterministic output
    reasoning_effort: low       # Reasoning effort level (low/medium/high)
  
  # Task planning model
  planner:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 4000            # Maximum tokens for planning (increased to prevent JSON truncation)
    model_name: gpt-oss-20b           # Model name for planning
    provider: openai            # Model provider
    temperature: 0.5            # Low temperature for consistent planning (JSON generation)
    reasoning_effort: low       # Reasoning effort level (low/medium/high)
  
  # Request routing model
  router:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 500             # Maximum tokens for routing decisions
    model_name: gpt-oss-20b   # Model name for routing
    provider: openai            # Model provider
    temperature: 0.5            # Low temperature for consistent routing
    reasoning_effort: low       # Reasoning effort level (low/medium/high)
  
  # Response synthesis model
  synthesizer:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 50000             # Maximum tokens for synthesis (optimized for detailed answers)
    model_name: gpt-oss-120b           # Model name for synthesis
    provider: openai            # Model provider
    temperature: 0.4             # Balanced temperature for natural, detailed responses
    reasoning_effort: low       # Reasoning effort level (low/medium/high)
  
  # Reasoning analysis model
  reasoning:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 50000            # Maximum tokens for detailed reasoning
    model_name: gpt-oss-20b           # Model name for reasoning
    provider: openai            # Model provider
    temperature: 0.7            # Balanced temperature for natural, detailed reasoning
    reasoning_effort: low       # Reasoning effort level (low/medium/high)
  
  # Productivity task model
  productivity:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 50000            # Maximum tokens for productivity tasks
    model_name: gpt-oss-20b   # Model name for productivity
    provider: openai            # Model provider
    temperature: 0.0            # Deterministic output for productivity
    reasoning_effort: low       # Reasoning effort level (low/medium/high)
  
  # Query generation model
  querymaker:
    fallback_enabled: true       # Enable fallback mechanisms
    max_tokens: 5000             # Maximum tokens for query generation
    model_name: gpt-oss-20b     # Model name for query generation
    provider: openai            # Model provider
    temperature: 0.3            # Higher temperature for diverse queries
    num_queries: 5              # Number of diverse queries to generate
    reasoning_effort: low       # Reasoning effort level (low/medium/high)

# Performance and optimization settings
performance:
  # Gunicorn server configuration
  gunicorn:
    worker_timeout: 300           # Worker timeout in seconds (5 minutes for complex IR_RAG queries)
    workers: null                 # Auto-calculate: CPU * 2 + 1
    worker_connections: 1000      # Max connections per worker
    max_requests: 1000            # Restart worker after N requests
    max_requests_jitter: 50       # Random jitter for worker restart
    keepalive: 5                  # Keep-alive timeout
    backlog: 2048                 # Max pending connections
    log_level: "info"             # Log level: debug, info, warning, error, critical
  
  # Caching configuration
  cache:
    enabled: false    # Enable response caching
    max_size: 1000   # Maximum cache entries
    ttl: 3600        # Time-to-live in seconds (1 hour)
  
  # Concurrency settings
  concurrency:
    max_parallel_requests: 5      # Maximum parallel API requests
    max_parallel_web_fetches: 3   # Maximum parallel web fetches
    max_concurrent_requests: 100  # Global concurrent request limit (Semaphore)
    llm_concurrent_limit: 20      # LLM concurrent request limit (Semaphore)
  
  # Timeout settings (in seconds)
  timeouts:
    action_execution: 120  # Timeout for action execution
    llm_request: 30       # Timeout for LLM requests
    web_request: 10       # Timeout for web requests
  
  # Token limits
  tokens:
    max_input: 50000   # Maximum input tokens
    max_output: 50000  # Maximum output tokens

# Productivity tools configuration
productivity:
  # Classification settings
  classification:
    confidence_threshold: 0.7  # Minimum confidence for classification
    fallback_to_rules: true    # Enable rule-based fallback
    use_llm: true             # Use LLM for classification
  
  # Output formatting
  output:
    auto_detect_kind: true      # Auto-detect content type
    include_metadata: true      # Include metadata in output
    include_statistics: true    # Include processing statistics
  
  # Available productivity tasks
  tasks:
    # Content analysis task
    analyze:
      enabled: true         # Enable analysis functionality
      max_insights: 5      # Maximum insights to generate
    
    # Information extraction task
    extract:
      enabled: true         # Enable extraction functionality
      supported_types:      # Types of information to extract
      - dates
      - names
      - numbers
      - emails
      - urls
    
    # Content formatting task
    format:
      enabled: true         # Enable formatting functionality
      supported_formats:    # Supported output formats
      - json
      - csv
      - markdown
      - html
    
    # Content rewriting task
    rewrite:
      default_tone: neutral  # Default writing tone
      enabled: true         # Enable rewriting functionality
      preserve_facts: true  # Preserve factual information
    
    # Content summarization task
    summarize:
      default_length: moderate    # Default summary length
      enabled: true              # Enable summarization functionality
      max_bullet_points: 10      # Maximum bullet points in summary
    
    # Translation task
    translate:
      enabled: true         # Enable translation functionality
      supported_languages:  # Supported language codes
      - en    # English
      - zh-cn # Chinese Simplified
      - zh-tw # Chinese Traditional
      - ja    # Japanese
      - ko    # Korean

# Reasoning engine configuration
reasoning:
  # Classification settings
  classification:
    confidence_threshold: 0.6  # Minimum confidence for reasoning classification
    fallback_to_rules: true    # Enable rule-based fallback
    use_model: false          # Use ML model for classification
  
  # Output configuration
  output:
    include_confidence: true        # Include confidence scores
    include_reasoning_chain: true   # Include reasoning steps
    max_length: 2000               # Maximum output length
  
  # Reasoning scaffolds (structured reasoning templates)
  scaffolds:
    # Analytical reasoning
    analytical:
      enabled: true                    # Enable analytical reasoning
      max_steps: 5                    # Maximum reasoning steps
      template: step_by_step_analysis # Template type
    
    # Comparative reasoning
    comparative:
      enabled: true              # Enable comparative reasoning
      max_comparisons: 5         # Maximum comparisons
      template: comparison_table # Template type
    
    # Explanatory reasoning
    explanatory:
      enabled: true                    # Enable explanatory reasoning
      max_factors: 3                   # Maximum factors to consider
      template: cause_effect_explanation # Template type
    
    # Predictive reasoning
    predictive:
      enabled: true              # Enable predictive reasoning
      template: trend_analysis   # Template type
      time_horizon: 2-5 years    # Prediction time horizon

# Action registry configuration
registry:
  # Available action types
  available_actions:
  - PRODUCTIVITY  # Productivity tools
  - REASONING     # Reasoning engine
  - IR_RAG        # Information retrieval
  
  # Fallback mappings for unavailable actions
  fallbacks:
    CREATIVE: REASONING     # Creative tasks fallback to reasoning
    IR_RAG: REASONING      # IR_RAG fallback to reasoning
    MATH: REASONING        # Math tasks fallback to reasoning
    MULTIMODAL: REASONING  # Multimodal tasks fallback to reasoning
    RESPONSE: REASONING    # Response tasks fallback to reasoning
  
  # Task type mappings
  mappings:
    CONVERSATIONAL_FOLLOWUP: RESPONSE      # Follow-up conversations
    CREATIVE_GENERATION: CREATIVE          # Creative content generation
    INFORMATION_RETRIEVAL: IR_RAG         # Information retrieval tasks
    KNOWLEDGE_REASONING: REASONING        # Knowledge-based reasoning
    MATH_QUERY: MATH                      # Mathematical queries
    MULTIMODAL_QUERY: MULTIMODAL          # Multimodal queries
    TASK_PRODUCTIVITY: PRODUCTIVITY       # Productivity tasks

# Request routing configuration
router:
  confidence_threshold: 0.55  # Minimum confidence for routing decisions
  
  # Feature extraction settings
  features:
    enable_reasoning_indicators: true  # Enable reasoning pattern detection
    enable_temporal_indicators: true  # Enable temporal pattern detection
    max_keywords: 20                  # Maximum keywords to extract
    min_keyword_length: 3             # Minimum keyword length
  
  # Heuristic detection settings
  heuristics:
    enable_creative_detection: true    # Enable creative task detection
    enable_multimodal_detection: true  # Enable multimodal task detection
    enable_obvious_math: true          # Enable obvious math detection
    enable_productivity_detection: true # Enable productivity task detection
  
  # LLM fallback settings
  llm_fallback:
    confidence_boost: 0.65  # Confidence boost for LLM decisions
    enabled: true           # Enable LLM fallback
  
  strategy: hybrid  # Routing strategy (hybrid, rule-based, llm-based)

# Geographic query configuration
geo_query:
  enable_enhancement: true  # Enable LLM-based response enhancement for friendlier output
  enhancement_temperature: 0.2  # Temperature for enhancement (higher = more creative)

bazi_query:
  enable_enhancement: true  # Enable LLM-based response enhancement for interpretations
  enhancement_temperature: 0.5  # Temperature for enhancement (balanced for cultural content)
  model_name: gpt-oss-20b  # Model for enhancement (cost-effective for concise responses)
  provider: openai  # Provider for the model

# Response length control by category
response_length:
  # Information Retrieval responses
  information_retrieval:
    max_tokens: 15000          # Maximum tokens for IR_RAG responses (comprehensive analysis)
    temperature: 0.2          # Temperature for IR_RAG
    
  # Knowledge Reasoning responses  
  knowledge_reasoning:
    max_tokens: 6000          # Maximum tokens for reasoning responses (detailed analysis)
    temperature: 0.7          # Temperature for reasoning
    
  # Geographic Query responses
  geo_query:
    max_tokens: 500           # Maximum tokens for geo queries (concise, friendly)
    temperature: 0.3          # Temperature for geo queries
    
  bazi_query:
    max_tokens: 2000          # Maximum tokens for bazi queries (detailed interpretations)
    temperature: 0.5          # Temperature for bazi queries (balanced)
    
  # Conversational Followup responses
  conversational_followup:
    max_tokens: 300           # Maximum tokens for simple followups (brief)
    temperature: 0.6          # Temperature for followups
    
  # Task Productivity responses
  task_productivity:
    max_tokens: 2000          # Maximum tokens for productivity tasks
    temperature: 0.0          # Deterministic for productivity
    
  # Default fallback
  default:
    max_tokens: 3000          # Default max tokens
    temperature: 0.7          # Default temperature

# System-wide configuration
system:
  debug: true                    # Enable debug mode
  default_timeout: 30           # Default timeout in seconds
  environment: development      # Environment (development, testing, production)
  max_execution_rounds: 2       # Maximum execution rounds per request
  name: WebChasor              # System name
  version: 1.0.0               # System version
