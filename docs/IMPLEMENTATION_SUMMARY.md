# å“åº”é•¿åº¦æ§åˆ¶å®æ–½æ€»ç»“

## âœ… å·²å®Œæˆçš„ä¿®æ”¹

### 1. **config.yaml é…ç½®** âœ…
- æ–‡ä»¶: `config/config.yaml`
- æ·»åŠ äº† `response_length` é…ç½®èŠ‚
- ä¸º 6 ç§ç±»åˆ«é…ç½®äº† `max_tokens` å’Œ `temperature`

```yaml
response_length:
  information_retrieval:
    max_tokens: 8000
    temperature: 0.8
  knowledge_reasoning:
    max_tokens: 6000
    temperature: 0.7
  geo_query:
    max_tokens: 500      # âœ… GEO_QUERY ç°åœ¨é™åˆ¶ä¸º 500 tokens
    temperature: 0.3
  conversational_followup:
    max_tokens: 300      # âœ… è¿½é—®é™åˆ¶ä¸º 300 tokens
    temperature: 0.6
  task_productivity:
    max_tokens: 2000
    temperature: 0.0
  default:
    max_tokens: 3000
    temperature: 0.7
```

### 2. **config_manager.py è¾…åŠ©æ–¹æ³•** âœ…
- æ–‡ä»¶: `src/config_manager.py`
- æ·»åŠ äº† `get_response_length_config()` æ–¹æ³•
- æ”¯æŒæŒ‰ç±»åˆ«è·å–é…ç½®ï¼Œå¸¦é»˜è®¤å€¼å›é€€

```python
def get_response_length_config(self, category: str) -> Dict[str, Any]:
    """Get response length configuration for a specific category"""
    category_key = category.lower()
    config_path = f'response_length.{category_key}'
    category_config = self.get(config_path, {})
    
    if not category_config:
        category_config = self.get('response_length.default', {
            'max_tokens': 3000,
            'temperature': 0.7
        })
    
    return category_config
```

### 3. **synthesizer.py æ ¸å¿ƒåŠŸèƒ½** âœ…
- æ–‡ä»¶: `src/synthesizer.py`
- ä¿®æ”¹äº† 3 ä¸ªå…³é”®æ–¹æ³•

#### 3.1 `_create_default_llm()` - æ”¯æŒåŠ¨æ€ max_tokens
```python
async def real_llm(prompt, temperature=0, max_tokens=None):
    if max_tokens is None:
        max_tokens = get_config().get('models.synthesizer.max_tokens', 2000)
    
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=max_tokens  # âœ… ä½¿ç”¨åŠ¨æ€ max_tokens
    )
    return response.choices[0].message.content
```

#### 3.2 `generate()` - ä» config è¯»å–é•¿åº¦é…ç½®
```python
async def generate(self, category, style_key, constraints, materials, task_scaffold=None):
    # 1. è‡ªåŠ¨è¯­è¨€æ£€æµ‹
    auto_lang = detect_lang_4way(materials)
    
    # 2. ä» config è·å–è¯¥ç±»åˆ«çš„é•¿åº¦é…ç½®
    cfg = get_config()
    length_config = cfg.get_response_length_config(category)
    
    # 3. ä½¿ç”¨ constraints ä¸­çš„å€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ config ä¸­çš„å€¼
    max_tokens = constraints.get('max_tokens', length_config.get('max_tokens', 3000))
    temperature = constraints.get('temperature', length_config.get('temperature', self.temperature))
    
    # 4. æ·»åŠ é•¿åº¦æç¤º
    instruction_hint = constraints.get('instruction_hint', '')
    if max_tokens <= 500:
        length_hint = f"\n\nIMPORTANT: Keep response concise and focused (approximately {max_tokens} tokens / {int(max_tokens * 0.75)} words)."
    # ...
    
    # 5. è°ƒç”¨ LLM
    response = await self.llm(prompt, temperature=temperature, max_tokens=max_tokens)
    return response
```

#### 3.3 `_mock_llm()` - æ”¯æŒ max_tokens å‚æ•°
```python
async def _mock_llm(self, prompt, temperature=0, max_tokens=None):
    # æµ‹è¯•æ—¶ä¹Ÿæ”¯æŒ max_tokens å‚æ•°
    ...
```

### 4. **prompt.py ç»Ÿä¸€æç¤ºè¯ç®¡ç†** âœ…
- æ–‡ä»¶: `src/prompt.py`
- æ·»åŠ äº†æç¤ºè¯å­—å…¸å’Œè¾…åŠ©å‡½æ•°

#### 4.1 IR_RAG æç¤ºè¯
```python
IR_RAG_INSTRUCTION_HINTS = {
    "default": """Provide a comprehensive answer with clear sections...""",
    "simple": """Provide a clear, well-structured answer (800-1000 words)...""",
    "moderate": """Provide a detailed answer (2000-3000 words)...""",
    "complex": """Provide a comprehensive analysis (6000-8000 words)..."""
}
```

#### 4.2 REASONING æç¤ºè¯
```python
REASONING_INSTRUCTION_HINTS = {
    "analytical": """Focus on ANALYTICAL reasoning approach...""",
    "comparative": """Focus on COMPARATIVE reasoning approach...""",
    "explanatory": """Focus on EXPLANATORY reasoning approach...""",
    "predictive": """Focus on PREDICTIVE reasoning approach...""",
    "problem_solving": """Focus on PROBLEM_SOLVING approach..."""
}
```

#### 4.3 GEO_QUERY æç¤ºè¯æ¨¡æ¿
```python
GEO_QUERY_INSTRUCTION_HINT = """User asked: "{user_query}"

Please rewrite the route information above to be more friendly and conversational while:
- Keeping the SAME language as the user's query
- Preserving ALL factual details (station names, distances, durations, transit lines)
- Making it easy to read and follow
- Adding a brief friendly greeting/closing if appropriate
- Keep it concise (150-200 words)"""
```

#### 4.4 è¾…åŠ©å‡½æ•°
```python
def get_length_hint(max_tokens: int) -> str:
    """Generate appropriate length hint based on max_tokens"""
    if max_tokens <= 500:
        return f"\n\nIMPORTANT: Keep response concise and focused..."
    # ...

def build_geo_query_instruction(user_query: str) -> str:
    """Build GEO_QUERY instruction hint with user query"""
    return GEO_QUERY_INSTRUCTION_HINT.format(user_query=user_query)
```

### 5. **geo_query.py é›†æˆ** âœ…
- æ–‡ä»¶: `src/actions/geo_query.py`
- ä¿®æ”¹äº† `_enhance_response()` æ–¹æ³•

```python
async def _enhance_response(self, result: Artifact, ctx: Context) -> Artifact:
    try:
        from prompt import build_geo_query_instruction, get_length_hint
        from config_manager import get_config
        
        # ä» config è·å–é•¿åº¦é…ç½®
        cfg = get_config()
        length_config = cfg.get_response_length_config("GEO_QUERY")
        max_tokens = length_config.get('max_tokens', 500)
        temperature = length_config.get('temperature', 0.3)
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æç¤ºè¯æ„å»ºå™¨
        instruction_hint = build_geo_query_instruction(ctx.query)
        instruction_hint += get_length_hint(max_tokens)
        
        # ä½¿ç”¨ synthesizer.generate() ç»Ÿä¸€æ¥å£
        enhanced_content = await self.synthesizer.generate(
            category="GEO_QUERY",
            style_key="auto",
            constraints={
                "language": "auto",
                "tone": "friendly, conversational",
                "temperature": temperature,
                "max_tokens": max_tokens,  # âœ… é™åˆ¶ä¸º 500 tokens
                "instruction_hint": instruction_hint
            },
            materials=materials,
            task_scaffold=None
        )
        
        print(f"[GEO_QUERY][ENHANCE] Completed (max_tokens={max_tokens})")
        
        # åœ¨ meta ä¸­è®°å½•é…ç½®ä¿¡æ¯
        enhanced_result = Artifact(
            kind=result.kind,
            content=enhanced_content,
            meta={
                **result.meta,
                "enhanced": True,
                "original_length": len(result.content),
                "enhanced_length": len(enhanced_content),
                "max_tokens": max_tokens,  # âœ… è®°å½•ä½¿ç”¨çš„ max_tokens
                "temperature": temperature
            }
        )
        
        return enhanced_result
        
    except Exception as e:
        # é”™è¯¯å¤„ç†...
```

---

## ğŸ¯ å®ç°æ•ˆæœ

### GEO_QUERY å­—æ•°æ§åˆ¶
- **é…ç½®**: `max_tokens: 500`
- **é¢„æœŸå­—æ•°**: ~375 å­—
- **æ¸©åº¦**: 0.3ï¼ˆæ›´ç¡®å®šæ€§ï¼Œæ›´ç®€æ´ï¼‰
- **æç¤ºè¯**: æ˜ç¡®è¦æ±‚ "Keep it concise (150-200 words)"

### å…¶ä»–ç±»åˆ«
| ç±»åˆ« | max_tokens | é¢„æœŸå­—æ•° | ç”¨é€” |
|------|------------|---------|------|
| INFORMATION_RETRIEVAL | 8000 | ~6000 å­— | ç»¼åˆåˆ†æ |
| KNOWLEDGE_REASONING | 6000 | ~4500 å­— | æ¨ç†åˆ†æ |
| **GEO_QUERY** | **500** | **~375 å­—** | **åœ°ç†æŸ¥è¯¢ï¼ˆç®€æ´ï¼‰** |
| CONVERSATIONAL_FOLLOWUP | 300 | ~225 å­— | ç®€å•è¿½é—® |
| TASK_PRODUCTIVITY | 2000 | ~1500 å­— | ç”Ÿäº§åŠ›ä»»åŠ¡ |

---

## ğŸ“‹ é…ç½®ä¼˜å…ˆçº§

ç³»ç»Ÿä½¿ç”¨ä¸‰çº§é…ç½®ä¼˜å…ˆçº§ï¼š

1. **constraints å‚æ•°** (æœ€é«˜ä¼˜å…ˆçº§)
   ```python
   constraints = {"max_tokens": 1000}  # è¦†ç›– config
   ```

2. **config.yaml é…ç½®** (ä¸­ç­‰ä¼˜å…ˆçº§)
   ```yaml
   response_length:
     geo_query:
       max_tokens: 500
   ```

3. **ä»£ç é»˜è®¤å€¼** (æœ€ä½ä¼˜å…ˆçº§)
   ```python
   max_tokens = 3000  # å¦‚æœå‰ä¸¤è€…éƒ½æ²¡æœ‰
   ```

---

## ğŸ§ª æµ‹è¯•æ–¹æ³•

### æµ‹è¯• GEO_QUERY å­—æ•°é™åˆ¶

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate webchaser

# è¿è¡Œæµ‹è¯•
python src/main.py
```

æµ‹è¯•æŸ¥è¯¢ï¼š
```
ä»ä¸­ç¯åˆ°å°–æ²™å’€æ€ä¹ˆèµ°ï¼Ÿ
```

é¢„æœŸç»“æœï¼š
- å›ç­”é•¿åº¦çº¦ 375 å­—ï¼ˆ500 tokensï¼‰
- åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼ˆç«™åã€è·ç¦»ã€æ—¶é•¿ï¼‰
- è¯­æ°”å‹å¥½ç®€æ´
- æœ‰é—®å€™å’Œç»“å°¾

æŸ¥çœ‹æ—¥å¿—ç¡®è®¤ï¼š
```
[GEO_QUERY][ENHANCE] Completed (max_tokens=500)
[SYNTHESIZER][EXEC] ... max_tokens=500
```

---

## â³ å¾…å®Œæˆçš„å·¥ä½œ

### 1. **ir_rag.py ä¿®æ”¹** (å¯é€‰)
å¦‚æœéœ€è¦ä¸º IR_RAG ä¹Ÿæ·»åŠ å­—æ•°æ§åˆ¶ï¼š

```python
# åœ¨ _synthesize_response() ä¸­
cfg = get_config()
length_config = cfg.get_response_length_config("INFORMATION_RETRIEVAL")
max_tokens = length_config.get('max_tokens', 8000)

constraints = {
    "max_tokens": max_tokens,
    "temperature": length_config.get('temperature', 0.8),
    "instruction_hint": IR_RAG_INSTRUCTION_HINTS["default"] + get_length_hint(max_tokens)
}
```

### 2. **reasoning.py ä¿®æ”¹** (å¯é€‰)
å¦‚æœéœ€è¦ä¸º REASONING ä¹Ÿæ·»åŠ å­—æ•°æ§åˆ¶ï¼š

```python
# åœ¨ run() ä¸­
cfg = get_config()
length_config = cfg.get_response_length_config("KNOWLEDGE_REASONING")
max_tokens = length_config.get('max_tokens', 6000)

constraints = {
    "max_tokens": max_tokens,
    "temperature": length_config.get('temperature', 0.7),
    "instruction_hint": REASONING_INSTRUCTION_HINTS[reasoning_key] + get_length_hint(max_tokens)
}
```

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

### ä¿®æ”¹ GEO_QUERY å­—æ•°é™åˆ¶

åªéœ€ä¿®æ”¹ `config/config.yaml`ï¼š

```yaml
response_length:
  geo_query:
    max_tokens: 300      # æ”¹ä¸ºæ›´çŸ­
    temperature: 0.2     # æ”¹ä¸ºæ›´ç¡®å®š
```

æ— éœ€ä¿®æ”¹ä»»ä½•ä»£ç ï¼

### ä¸ºæ–°ç±»åˆ«æ·»åŠ å­—æ•°æ§åˆ¶

1. åœ¨ `config.yaml` ä¸­æ·»åŠ é…ç½®ï¼š
```yaml
response_length:
  my_new_category:
    max_tokens: 1000
    temperature: 0.5
```

2. åœ¨ action ä¸­ä½¿ç”¨ï¼š
```python
cfg = get_config()
length_config = cfg.get_response_length_config("MY_NEW_CATEGORY")
max_tokens = length_config.get('max_tokens', 3000)
```

---

## ğŸ‰ æ€»ç»“

### å·²å®ç°çš„æ ¸å¿ƒåŠŸèƒ½
1. âœ… é…ç½®æ–‡ä»¶ç»Ÿä¸€ç®¡ç†å“åº”é•¿åº¦
2. âœ… Synthesizer æ”¯æŒåŠ¨æ€ max_tokens
3. âœ… æç¤ºè¯ç»Ÿä¸€ç®¡ç†åœ¨ prompt.py
4. âœ… GEO_QUERY æˆåŠŸé›†æˆå­—æ•°æ§åˆ¶
5. âœ… å®Œæ•´çš„é…ç½®ä¼˜å…ˆçº§æœºåˆ¶

### å…³é”®æ”¹è¿›
- **é›†ä¸­é…ç½®**: æ‰€æœ‰é•¿åº¦æ§åˆ¶åœ¨ config.yaml
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ action ä½¿ç”¨ synthesizer.generate()
- **æç¤ºè¯ç®¡ç†**: æ‰€æœ‰æç¤ºè¯åœ¨ prompt.py
- **çµæ´»æ‰©å±•**: æ˜“äºæ·»åŠ æ–°ç±»åˆ«
- **å‘åå…¼å®¹**: ä¿ç•™ constraints å‚æ•°è¦†ç›–

### GEO_QUERY æ•ˆæœ
- âœ… ä»ä¹‹å‰å¯èƒ½çš„é•¿å›ç­”é™åˆ¶åˆ° ~375 å­—
- âœ… ä¿æŒæ‰€æœ‰å…³é”®ä¿¡æ¯
- âœ… æ›´ç®€æ´å‹å¥½çš„è¡¨è¾¾
- âœ… å¯é€šè¿‡é…ç½®æ–‡ä»¶è½»æ¾è°ƒæ•´

---

æœ€åæ›´æ–°: 2025-10-06
ä½œè€…: AI Assistant
çŠ¶æ€: âœ… æ ¸å¿ƒåŠŸèƒ½å·²å®Œæˆå¹¶å¯æµ‹è¯•
