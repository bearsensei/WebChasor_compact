# Response Length Control Implementation

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•é€šè¿‡ `config.yaml` ç»Ÿä¸€æ§åˆ¶ä¸åŒ Router ç±»åˆ«çš„å›ç­”é•¿åº¦å’Œæ¸©åº¦å‚æ•°ã€‚

---

## âœ… å·²å®Œæˆçš„ä¿®æ”¹

### 1. **config.yaml é…ç½®æ·»åŠ ** âœ…

åœ¨ `config/config.yaml` ä¸­æ·»åŠ äº†æ–°çš„ `response_length` é…ç½®èŠ‚ï¼š

```yaml
# Response length control by category
response_length:
  # Information Retrieval responses
  information_retrieval:
    max_tokens: 8000          # ç»¼åˆåˆ†æç±»é—®é¢˜ï¼ˆè¶…é•¿å›ç­”ï¼‰
    temperature: 0.8
    
  # Knowledge Reasoning responses  
  knowledge_reasoning:
    max_tokens: 6000          # æ¨ç†åˆ†æç±»é—®é¢˜ï¼ˆé•¿å›ç­”ï¼‰
    temperature: 0.7
    
  # Geographic Query responses
  geo_query:
    max_tokens: 500           # åœ°ç†æŸ¥è¯¢ï¼ˆç®€æ´ï¼‰
    temperature: 0.3
    
  # Conversational Followup responses
  conversational_followup:
    max_tokens: 300           # ç®€å•è¿½é—®ï¼ˆç®€çŸ­ï¼‰
    temperature: 0.6
    
  # Task Productivity responses
  task_productivity:
    max_tokens: 2000          # ç”Ÿäº§åŠ›ä»»åŠ¡
    temperature: 0.0
    
  # Default fallback
  default:
    max_tokens: 3000
    temperature: 0.7
```

### 2. **config_manager.py è¾…åŠ©æ–¹æ³•æ·»åŠ ** âœ…

åœ¨ `src/config_manager.py` ä¸­æ·»åŠ äº† `get_response_length_config()` æ–¹æ³•ï¼š

```python
def get_response_length_config(self, category: str) -> Dict[str, Any]:
    """
    Get response length configuration for a specific category.
    
    Args:
        category: Router category (e.g., 'INFORMATION_RETRIEVAL', 'GEO_QUERY')
    
    Returns:
        Dict with max_tokens and temperature
    """
    # Normalize category name to config key
    category_key = category.lower()
    
    # Get category-specific config
    config_path = f'response_length.{category_key}'
    category_config = self.get(config_path, {})
    
    # Fallback to default if not found
    if not category_config:
        category_config = self.get('response_length.default', {
            'max_tokens': 3000,
            'temperature': 0.7
        })
    
    return category_config
```

---

## ğŸ”§ å¾…å®Œæˆçš„ä¿®æ”¹

### 3. **synthesizer.py ä¿®æ”¹** â³

éœ€è¦ä¿®æ”¹ `src/synthesizer.py` çš„ä»¥ä¸‹éƒ¨åˆ†ï¼š

#### 3.1 ä¿®æ”¹ `_create_default_llm()` æ–¹æ³•

```python
def _create_default_llm(self):
    """Create default LLM from environment variables"""
    api_base = get_config().get('external_services.openai.api_base', 'https://api.openai.com/v1')
    api_key = os.getenv("OPENAI_API_KEY_AGENT") 
    model = get_config().get('models.synthesizer.model_name', 'gpt-4')
    
    if not api_base or not api_key:
        print("[SYNTHESIZER][WARN] No API credentials, using mock LLM")
        return self._mock_llm
    
    try:
        import openai
        client = openai.OpenAI(api_key=api_key, base_url=api_base)
        
        # ä¿®æ”¹ï¼šæ”¯æŒåŠ¨æ€ max_tokens
        async def real_llm(prompt, temperature=0, max_tokens=None):
            # å¦‚æœæ²¡æœ‰æŒ‡å®š max_tokensï¼Œä½¿ç”¨ config ä¸­çš„é»˜è®¤å€¼
            if max_tokens is None:
                max_tokens = get_config().get('models.synthesizer.max_tokens', 2000)
            
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens  # ä½¿ç”¨ä¼ å…¥çš„ max_tokens
            )
            return response.choices[0].message.content
        
        return real_llm
        
    except ImportError:
        print("âš ï¸ OpenAI library not available, using mock LLM")
        return self._mock_llm
    except Exception as e:
        print(f"âš ï¸ Failed to initialize OpenAI client: {e}, using mock LLM")
        return self._mock_llm
```

#### 3.2 ä¿®æ”¹ `generate()` æ–¹æ³•

```python
async def generate(self, category, style_key, constraints, materials, task_scaffold=None):
    # 1. è‡ªåŠ¨è¯­è¨€æ£€æµ‹
    auto_lang = detect_lang_4way(materials)
    print(f'[SYNTHESIZER][LANG_DEBUG] auto_lang: {auto_lang}')
    
    # 2. ä» config è·å–è¯¥ç±»åˆ«çš„é•¿åº¦é…ç½®
    cfg = get_config()
    length_config = cfg.get_response_length_config(category)
    
    # 3. ä½¿ç”¨ constraints ä¸­çš„å€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ config ä¸­çš„å€¼
    max_tokens = constraints.get('max_tokens', length_config.get('max_tokens', 3000))
    temperature = constraints.get('temperature', length_config.get('temperature', self.temperature))
    
    # 4. æ„å»º promptï¼ˆæ·»åŠ é•¿åº¦æç¤ºï¼‰
    instruction_hint = constraints.get('instruction_hint', '')
    
    # æ ¹æ® max_tokens æ·»åŠ é•¿åº¦æç¤º
    if max_tokens <= 500:
        length_hint = f"\n\nIMPORTANT: Keep response concise and focused (approximately {max_tokens} tokens / {int(max_tokens * 0.75)} words)."
    elif max_tokens <= 2000:
        length_hint = f"\n\nIMPORTANT: Provide a well-structured response (approximately {max_tokens} tokens / {int(max_tokens * 0.75)} words)."
    else:
        length_hint = f"\n\nIMPORTANT: Provide a comprehensive response (approximately {max_tokens} tokens / {int(max_tokens * 0.75)} words)."
    
    instruction_hint = instruction_hint + length_hint
    
    # 5. æ¸²æŸ“ prompt
    prompt = render_synthesizer_prompt(
        action_policy=SYNTHESIZER_ACTION_POLICIES.get(category, "Provide helpful and accurate responses."),
        materials=materials,
        user_query=materials,
        language=auto_lang,
        reading_level=constraints.get("reading_level", "general"),
        preferred_style=(style_key if style_key and style_key != "auto" else None),
        global_system=SYNTHESIZER_GLOBAL_SYSTEM,
        internal_scaffold=(SYNTHESIZER_HIDDEN_REASONING_SCAFFOLD if category == "KNOWLEDGE_REASONING" else ""),
        instruction_hint=instruction_hint
    )

    print(f"[SYNTHESIZER][EXEC] model={self.model_name} temp={temperature} category={category} lang={auto_lang} max_tokens={max_tokens}")

    # 6. è°ƒç”¨ LLMï¼ˆä¼ å…¥ max_tokensï¼‰
    response = await self.llm(prompt, temperature=temperature, max_tokens=max_tokens)
    return response
```

### 4. **prompt.py æ·»åŠ æç¤ºè¯ç®¡ç†** â³

åœ¨ `src/prompt.py` æ–‡ä»¶æœ«å°¾æ·»åŠ ï¼š

```python
# ============================================================================
# ACTION-SPECIFIC INSTRUCTION HINTS
# ============================================================================

# IR_RAG instruction hints
IR_RAG_INSTRUCTION_HINTS = {
    "default": """Provide a comprehensive answer with clear sections. Use diverse formatting: bullet points, numbered lists, and tables where appropriate. Mix paragraphs with structured formats for better readability. DO NOT use citation numbers [1], [2], etc. in the main text. Only include a reference list at the end.""",
    
    "simple": """Provide a clear, well-structured answer (800-1000 words) with 3-4 key sections. Use bullet points and tables where helpful.""",
    
    "moderate": """Provide a detailed answer (2000-3000 words) with 5-6 sections. Mix paragraphs with lists and tables for better readability.""",
    
    "complex": """Provide a comprehensive analysis (6000-8000 words) with 8-10 detailed sections. Use diverse formatting: paragraphs, bullet points, numbered lists, comparison tables, and structured data presentations."""
}

# REASONING instruction hints
REASONING_INSTRUCTION_HINTS = {
    "analytical": """Focus on ANALYTICAL reasoning approach. Use diverse formatting: bullet points, numbered lists, comparison tables, and structured presentations where appropriate. Mix paragraphs with lists and tables for better readability.""",
    
    "comparative": """Focus on COMPARATIVE reasoning approach. Use comparison tables, pros/cons lists, and side-by-side analysis. Highlight key differences and similarities.""",
    
    "explanatory": """Focus on EXPLANATORY reasoning approach. Break down complex concepts into clear steps. Use diagrams descriptions, examples, and analogies where helpful.""",
    
    "predictive": """Focus on PREDICTIVE reasoning approach. Analyze trends, present scenarios, and discuss probabilities. Use data visualization descriptions where helpful.""",
}

# GEO_QUERY instruction hint
GEO_QUERY_INSTRUCTION_HINT = """User asked: "{user_query}"

Please rewrite the route information above to be more friendly and conversational while:
- Keeping the SAME language as the user's query
- Preserving ALL factual details (station names, distances, durations, transit lines)
- Making it easy to read and follow
- Adding a brief friendly greeting/closing if appropriate
- Keep it concise (150-200 words)"""

# Helper function
def get_length_hint(max_tokens: int) -> str:
    """Generate appropriate length hint based on max_tokens"""
    if max_tokens <= 500:
        words = int(max_tokens * 0.75)
        return f"\n\nIMPORTANT: Keep response concise and focused (approximately {max_tokens} tokens / {words} words)."
    elif max_tokens <= 2000:
        words = int(max_tokens * 0.75)
        return f"\n\nIMPORTANT: Provide a well-structured response (approximately {max_tokens} tokens / {words} words)."
    else:
        words = int(max_tokens * 0.75)
        return f"\n\nIMPORTANT: Provide a comprehensive response (approximately {max_tokens} tokens / {words} words)."

def build_geo_query_instruction(user_query: str) -> str:
    """Build GEO_QUERY instruction hint"""
    return GEO_QUERY_INSTRUCTION_HINT.format(user_query=user_query)
```

### 5. **ir_rag.py ä¿®æ”¹** â³

åœ¨ `src/actions/ir_rag.py` çš„ `_synthesize_response()` æ–¹æ³•ä¸­ï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¯¼å…¥
from prompt import IR_RAG_INSTRUCTION_HINTS, get_length_hint

# åœ¨ _synthesize_response() ä¸­ä¿®æ”¹
async def _synthesize_response(self, ctx: Context, plan: ExtractionPlan, 
                             extracted_vars: Dict[str, ExtractedVariable], 
                             search_results: List[SearchResult], toolset) -> Artifact:
    
    # è·å–é…ç½®
    cfg = get_config()
    length_config = cfg.get_response_length_config("INFORMATION_RETRIEVAL")
    max_tokens = length_config.get('max_tokens', 8000)
    
    # ä½¿ç”¨ç»Ÿä¸€çš„ instruction hint
    instruction_hint = IR_RAG_INSTRUCTION_HINTS["default"]
    instruction_hint += get_length_hint(max_tokens)
    
    constraints = {
        "language": "auto",
        "tone": "factual, authoritative",
        "temperature": length_config.get('temperature', 0.8),
        "max_tokens": max_tokens,
        "instruction_hint": instruction_hint
    }
    
    response = await toolset.synthesizer.generate(
        category="INFORMATION_RETRIEVAL",
        style_key="auto",
        constraints=constraints,
        materials=f"Query: {ctx.query}\n\nExtracted Information:\n{materials}",
        task_scaffold=None
    )
    
    # ... å…¶ä½™ä»£ç 
```

### 6. **reasoning.py ä¿®æ”¹** â³

åœ¨ `src/actions/reasoning.py` çš„ `run()` æ–¹æ³•ä¸­ï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¯¼å…¥
from prompt import REASONING_INSTRUCTION_HINTS, get_length_hint

# åœ¨ run() ä¸­ä¿®æ”¹
async def run(self, ctx: Context, toolset) -> Artifact:
    # ... å‰é¢çš„ä»£ç 
    
    # è·å–é…ç½®
    cfg = get_config()
    length_config = cfg.get_response_length_config("KNOWLEDGE_REASONING")
    max_tokens = length_config.get('max_tokens', 6000)
    
    # é€‰æ‹© instruction hint
    reasoning_key = reasoning_type.value.lower()
    instruction_hint = REASONING_INSTRUCTION_HINTS.get(reasoning_key, REASONING_INSTRUCTION_HINTS["analytical"])
    instruction_hint += get_length_hint(max_tokens)
    
    constraints = {
        "language": "auto",
        "tone": "friendly, conversational",
        "temperature": length_config.get('temperature', 0.7),
        "max_tokens": max_tokens,
        "instruction_hint": instruction_hint
    }
    
    text = await toolset.synthesizer.generate(
        category="KNOWLEDGE_REASONING",
        style_key="auto",
        constraints=constraints,
        materials=ctx.query,
        task_scaffold=None
    )
    
    # ... å…¶ä½™ä»£ç 
```

### 7. **geo_query.py ä¿®æ”¹** â³

åœ¨ `src/actions/geo_query.py` çš„ `_enhance_response()` æ–¹æ³•ä¸­ï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¯¼å…¥
from prompt import build_geo_query_instruction, get_length_hint

# åœ¨ _enhance_response() ä¸­ä¿®æ”¹
async def _enhance_response(self, result: Artifact, ctx: Context) -> Artifact:
    print(f"[GEO_QUERY][ENHANCE] Starting enhancement...")
    
    try:
        # è·å–é…ç½®
        cfg = get_config()
        length_config = cfg.get_response_length_config("GEO_QUERY")
        max_tokens = length_config.get('max_tokens', 500)
        
        # ä½¿ç”¨ç»Ÿä¸€çš„ instruction hint
        instruction_hint = build_geo_query_instruction(ctx.query)
        instruction_hint += get_length_hint(max_tokens)
        
        # ä½¿ç”¨ synthesizer.generate() è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ llm()
        enhanced_content = await self.synthesizer.generate(
            category="GEO_QUERY",
            style_key="auto",
            constraints={
                "language": "auto",
                "tone": "friendly, conversational",
                "temperature": length_config.get('temperature', 0.3),
                "max_tokens": max_tokens,
                "instruction_hint": instruction_hint
            },
            materials=f"# Original Route Information\n\n{result.content}",
            task_scaffold=None
        )
        
        # åˆ›å»ºæ–°çš„ artifact
        enhanced_result = Artifact(
            kind=result.kind,
            content=enhanced_content,
            meta={
                **result.meta,
                "enhanced": True,
                "original_length": len(result.content),
                "enhanced_length": len(enhanced_content),
                "max_tokens": max_tokens
            }
        )
        
        return enhanced_result
        
    except Exception as e:
        # ... é”™è¯¯å¤„ç†
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®æ”¹å®Œæˆåï¼Œä¸åŒç±»å‹çš„æŸ¥è¯¢å°†æœ‰ä¸åŒçš„å›ç­”é•¿åº¦ï¼š

| Router Category | max_tokens | é¢„æœŸå­—æ•° | ç”¨é€” |
|----------------|------------|---------|------|
| **INFORMATION_RETRIEVAL** | 8000 | ~6000 å­— | ç»¼åˆåˆ†æç±»é—®é¢˜ |
| **KNOWLEDGE_REASONING** | 6000 | ~4500 å­— | æ¨ç†åˆ†æç±»é—®é¢˜ |
| **GEO_QUERY** | 500 | ~375 å­— | åœ°ç†æŸ¥è¯¢ï¼ˆç®€æ´ï¼‰ |
| **CONVERSATIONAL_FOLLOWUP** | 300 | ~225 å­— | ç®€å•è¿½é—® |
| **TASK_PRODUCTIVITY** | 2000 | ~1500 å­— | ç”Ÿäº§åŠ›ä»»åŠ¡ |

---

## ğŸ§ª æµ‹è¯•æ–¹æ³•

ä¿®æ”¹å®Œæˆåï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æµ‹è¯•ï¼š

```bash
# æµ‹è¯•åœ°ç†æŸ¥è¯¢ï¼ˆåº”è¯¥ç®€çŸ­ï¼‰
python src/main.py
# è¾“å…¥: "ä»ä¸­ç¯åˆ°å°–æ²™å’€æ€ä¹ˆèµ°ï¼Ÿ"
# é¢„æœŸ: ~500 tokens çš„ç®€æ´å›ç­”

# æµ‹è¯•ä¿¡æ¯æ£€ç´¢ï¼ˆåº”è¯¥è¯¦ç»†ï¼‰
# è¾“å…¥: "é¦™æ¸¯ä»Šå¹´ä¸­ç§‹æ´»åŠ¨"
# é¢„æœŸ: ~8000 tokens çš„è¯¦ç»†å›ç­”

# æµ‹è¯•æ¨ç†ï¼ˆåº”è¯¥ä¸­ç­‰é•¿åº¦ï¼‰
# è¾“å…¥: "ä¸ºä»€ä¹ˆé¦™æ¸¯æˆ¿ä»·è¿™ä¹ˆé«˜ï¼Ÿ"
# é¢„æœŸ: ~6000 tokens çš„åˆ†æå›ç­”
```

---

## ğŸ”„ åç»­ä¼˜åŒ–

1. **åŠ¨æ€å¤æ‚åº¦åˆ¤æ–­**: æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹© simple/moderate/complex
2. **ç”¨æˆ·åå¥½**: å…è®¸ç”¨æˆ·åœ¨æŸ¥è¯¢ä¸­æŒ‡å®šå›ç­”é•¿åº¦ï¼ˆ"ç®€çŸ­å›ç­”"/"è¯¦ç»†å›ç­”"ï¼‰
3. **A/B æµ‹è¯•**: æµ‹è¯•ä¸åŒé•¿åº¦å¯¹ç”¨æˆ·æ»¡æ„åº¦çš„å½±å“
4. **ç›‘æ§ç»Ÿè®¡**: è®°å½•å®é™…ç”Ÿæˆçš„ token æ•°ä¸é…ç½®çš„å·®å¼‚

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **max_tokens æ˜¯ä¸Šé™**: LLM å¯èƒ½ç”Ÿæˆæ›´çŸ­çš„å›ç­”ï¼Œä½†ä¸ä¼šè¶…è¿‡è¿™ä¸ªé™åˆ¶
2. **æ¸©åº¦å‚æ•°**: è¾ƒä½çš„æ¸©åº¦ï¼ˆ0.3ï¼‰é€‚åˆäº‹å®æ€§å›ç­”ï¼Œè¾ƒé«˜çš„æ¸©åº¦ï¼ˆ0.8ï¼‰é€‚åˆåˆ›é€ æ€§å›ç­”
3. **é…ç½®ä¼˜å…ˆçº§**: `constraints` å‚æ•° > `config.yaml` > ä»£ç é»˜è®¤å€¼
4. **å‘åå…¼å®¹**: å¦‚æœ `response_length` é…ç½®ä¸å­˜åœ¨ï¼Œä¼šä½¿ç”¨é»˜è®¤å€¼

---

æœ€åæ›´æ–°: 2025-10-06
