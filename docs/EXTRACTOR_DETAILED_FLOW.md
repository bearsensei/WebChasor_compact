# InformationExtractor è¯¦ç»†å·¥ä½œæµç¨‹

## æ•´ä½“æ¶æ„

```
InformationExtractor
â”œâ”€â”€ extract_variables()          # ä¸»å…¥å£ï¼šæå–æ‰€æœ‰å˜é‡
â”œâ”€â”€ _extract_single_variable()   # æå–å•ä¸ªå˜é‡
â”œâ”€â”€ _llm_extract()              # ä½¿ç”¨ LLM æå–ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
â”œâ”€â”€ _fallback_extract()         # Fallback æå–ï¼ˆç®€å•å¯å‘å¼ï¼‰
â”œâ”€â”€ _extract_core_entities()    # æå–æ ¸å¿ƒå®ä½“
â”œâ”€â”€ _extract_biographical_info() # æå–ä¼ è®°ä¿¡æ¯
â””â”€â”€ _extract_factual_claim()    # æå–äº‹å®å£°æ˜
```

---

## å®Œæ•´å·¥ä½œæµç¨‹

### ç¬¬ 1 æ­¥ï¼šå…¥å£ - `extract_variables()`

**ä½ç½®**: ç¬¬ 553-571 è¡Œ

```python
async def extract_variables(self, plan: ExtractionPlan, ranked_passages: Dict[str, List[ContentPassage]]) -> Dict[str, ExtractedVariable]:
    """Extract variables from ranked passages using LLM"""
    print(f"ğŸ” EXTRACTOR: Extracting {len(plan.tasks_to_extract)} variables...")
    
    extracted_vars = {}
    
    for task in plan.tasks_to_extract:  # éå†æ‰€æœ‰ extraction tasks
        passages = ranked_passages.get(task.variable_name, [])
        if not passages:
            print(f"ğŸ” EXTRACTOR: No passages found for {task.variable_name}")
            continue
        
        # Extract variable using LLM or fallback methods
        extracted_var = await self._extract_single_variable(task, passages)
        extracted_vars[task.variable_name] = extracted_var
        
        print(f"ğŸ” EXTRACTOR: Extracted '{task.variable_name}' with confidence {extracted_var.confidence:.2f}")
    
    return extracted_vars
```

**è¾“å…¥**:
- `plan`: ExtractionPlanï¼ˆåŒ…å«å¤šä¸ª tasksï¼‰
  ```python
  plan.tasks_to_extract = [
      PlanTask(fact="éƒ­æ¯…å¯çš„å­¦æœ¯èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ", variable_name="guoyi_profile", category="biography"),
      PlanTask(fact="éƒ­æ¯…å¯ç°ä»»èŒä½æ˜¯ä»€ä¹ˆï¼Ÿ", variable_name="guoyi_current_position", category="fact"),
      ...
  ]
  ```

- `ranked_passages`: æ¯ä¸ª task å¯¹åº”çš„ ranked passages
  ```python
  {
      "guoyi_profile": [Passage1, Passage2, ..., Passage20],
      "guoyi_current_position": [Passage1, Passage2, ..., Passage20],
      ...
  }
  ```

**è¾“å‡º**:
```python
{
    "guoyi_profile": ExtractedVariable(value="...", confidence=0.8, ...),
    "guoyi_current_position": ExtractedVariable(value="...", confidence=0.5, ...),
    ...
}
```

**æµç¨‹**:
```
For each task in plan:
  1. è·å–è¯¥ task çš„ ranked passages
  2. è°ƒç”¨ _extract_single_variable(task, passages)
  3. å­˜å‚¨ç»“æœåˆ° extracted_vars
  4. æ‰“å° confidence
```

---

### ç¬¬ 2 æ­¥ï¼šå•å˜é‡æå– - `_extract_single_variable()`

**ä½ç½®**: ç¬¬ 573-582 è¡Œ

```python
async def _extract_single_variable(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
    """Extract a single variable from passages"""
    try:
        if self.client:  # å¦‚æœæœ‰ LLM client
            return await self._llm_extract(task, passages)
        else:  # å¦‚æœæ²¡æœ‰ LLM client
            return self._fallback_extract(task, passages)
    except Exception as e:  # å¦‚æœ LLM æå–å¤±è´¥
        logger.error(f"Extraction failed for {task.variable_name}: {e}")
        return self._fallback_extract(task, passages)
```

**å†³ç­–æ ‘**:
```
æœ‰ LLM client?
â”œâ”€ Yes â†’ è°ƒç”¨ _llm_extract()
â”‚         â”œâ”€ æˆåŠŸ â†’ è¿”å› ExtractedVariable
â”‚         â””â”€ å¤±è´¥ (Exception) â†’ è°ƒç”¨ _fallback_extract()
â”‚
â””â”€ No â†’ ç›´æ¥è°ƒç”¨ _fallback_extract()
```

**å…³é”®ç‚¹**:
- ä¼˜å…ˆä½¿ç”¨ LLM æå–
- å¦‚æœå¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ° fallback
- **è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¼šå‡ºç° confidence 0.6 çš„åŸå› **

---

### ç¬¬ 3 æ­¥Aï¼šLLM æå– - `_llm_extract()` (ä¸»è·¯å¾„)

**ä½ç½®**: ç¬¬ 610-694 è¡Œ

#### 3A.1 å‡†å¤‡é˜¶æ®µ

```python
async def _llm_extract(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
    # 1. è·å–é…ç½®
    cfg = get_config()
    max_passages = cfg.get('ir_rag.content.max_passages_per_task', 7)  # ç°åœ¨æ˜¯ 20
    
    # 2. åˆå¹¶ passages
    combined_text = "\n\n".join([
        f"Source {i+1}: {p.source_url}\n{p.text}" 
        for i, p in enumerate(passages[:max_passages])
    ])
    
    # 3. æå–æ ¸å¿ƒå®ä½“
    core_entities = self._extract_core_entities(task.fact)
    entities_str = ", ".join(core_entities) if core_entities else "N/A"
```

**ä¾‹å­**:
```
task.fact = "éƒ­æ¯…å¯ä»€ä¹ˆæ—¶å€™å¯ä»¥å½“æµ¸ä¼šå¤§å­¦æ ¡é•¿ï¼Ÿ"

core_entities = ["éƒ­æ¯…å¯", "ä»€ä¹ˆæ—¶å€™", "å¯ä»¥", "æµ¸ä¼šå¤§å­¦", "æ ¡é•¿"]
                  â†“ (å–å‰5ä¸ª)
entities_str = "éƒ­æ¯…å¯, ä»€ä¹ˆæ—¶å€™, å¯ä»¥, æµ¸ä¼šå¤§å­¦, æ ¡é•¿"

combined_text = """
Source 1: https://...
éƒ­æ¯…å¯æ•™æˆç°ä»»...

Source 2: https://...
æµ¸ä¼šå¤§å­¦æ ¡é•¿ä»»æœŸ...

...

Source 20: https://...
"""
```

#### 3A.2 æ„å»º Prompt

```python
prompt = f"""
Extract the following information from the provided text passages:

QUESTION: {task.fact}
VARIABLE: {task.variable_name}
CATEGORY: {task.category}
CORE ENTITIES: {entities_str}

TEXT PASSAGES:
{combined_text}

**CRITICAL REQUIREMENTS**:
1. Extract information ONLY from the provided text passages above
2. DO NOT use your own knowledge or make assumptions
3. The extracted information MUST mention or relate to the CORE ENTITIES listed above
4. If multiple passages provide different information, prioritize the most specific and recent one
5. Quote the original text when possible to support your extraction

Please provide a JSON response with:
{{
    "value": "extracted information (or null if not found)",
    "confidence": 0.0-1.0,
    "reasoning": "brief explanation with source reference (e.g., 'Source 2 states...')",
    "source_quote": "direct quote from the passage supporting this extraction"
}}

If the information is not found in the passages, set confidence to 0.0 and value to null.
If the information does not relate to the CORE ENTITIES, set confidence to 0.0 and value to null.
"""
```

**å®é™…ä¾‹å­**:
```
Extract the following information from the provided text passages:

QUESTION: éƒ­æ¯…å¯ä»€ä¹ˆæ—¶å€™å¯ä»¥å½“æµ¸ä¼šå¤§å­¦æ ¡é•¿ï¼Ÿ
VARIABLE: guoyi_appointment_time
CATEGORY: fact
CORE ENTITIES: éƒ­æ¯…å¯, ä»€ä¹ˆæ—¶å€™, å¯ä»¥, æµ¸ä¼šå¤§å­¦, æ ¡é•¿

TEXT PASSAGES:
Source 1: https://www.hkbu.edu.hk/...
éƒ­æ¯…å¯æ•™æˆç°ä»»é¦™æ¸¯æµ¸ä¼šå¤§å­¦å‰¯æ ¡é•¿...

Source 2: https://news.mingpao.com/...
æµ¸ä¼šå¤§å­¦æ ¡é•¿ä»»æœŸé€šå¸¸ä¸º5å¹´...

Source 3: https://www.scmp.com/...
æ¸¯ç§‘å¤§æ ¡é•¿å¶ç‰å¦‚ä»»æœŸè‡³2026å¹´...  â† âš ï¸ é”™è¯¯å®ä½“

...

**CRITICAL REQUIREMENTS**:
1. Extract information ONLY from the provided text passages above
2. DO NOT use your own knowledge or make assumptions
3. The extracted information MUST mention or relate to the CORE ENTITIES listed above
...
```

#### 3A.3 è°ƒç”¨ LLM

```python
response = self.client.chat.completions.create(
    model=self.model_name,  # é»˜è®¤ gpt-3.5-turbo
    messages=[
        {"role": "system", "content": "You are an expert information extractor. Extract precise, factual information from text passages."},
        {"role": "user", "content": prompt}
    ],
    temperature=0.1,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
    max_tokens=500
)

result_text = response.choices[0].message.content.strip()
```

**LLM å¯èƒ½çš„å“åº”**:

**æƒ…å†µ 1: æ­£å¸¸ JSON å“åº”**
```json
{
    "value": "éƒ­æ¯…å¯æ•™æˆå°†äº2025å¹´7æœˆæ­£å¼æ¥ä»»æµ¸ä¼šå¤§å­¦æ ¡é•¿",
    "confidence": 0.85,
    "reasoning": "Source 2 states that Guo Yike will assume the position in July 2025",
    "source_quote": "éƒ­æ¯…å¯æ•™æˆå°†äº2025å¹´7æœˆæ­£å¼æ¥ä»»æµ¸ä¼šå¤§å­¦æ ¡é•¿ä¸€èŒ"
}
```

**æƒ…å†µ 2: æœªæ‰¾åˆ°ä¿¡æ¯**
```json
{
    "value": null,
    "confidence": 0.0,
    "reasoning": "None of the provided passages mention when Guo Yike will become president of HKBU"
}
```

**æƒ…å†µ 3: é JSON å“åº”ï¼ˆé—®é¢˜ï¼ï¼‰**
```
æ ¹æ®æä¾›çš„æ–‡æœ¬ï¼Œéƒ­æ¯…å¯æ•™æˆç°ä»»é¦™æ¸¯æµ¸ä¼šå¤§å­¦å‰¯æ ¡é•¿...
```
â†’ è¿™ä¼šå¯¼è‡´ `json.JSONDecodeError`

**æƒ…å†µ 4: None å“åº”ï¼ˆé—®é¢˜ï¼ï¼‰**
```python
response.choices[0].message.content = None
```
â†’ è¿™ä¼šå¯¼è‡´ `'NoneType' object has no attribute 'strip'`

#### 3A.4 è§£æå“åº”

```python
try:
    result_data = json.loads(result_text)
    
    # Log extraction details for debugging
    if cfg.is_decision_logging_enabled('ir_rag'):
        print(f"[EXTRACTOR][DEBUG] Extracted value: {result_data.get('value')}")
        print(f"[EXTRACTOR][DEBUG] Confidence: {result_data.get('confidence')}")
        print(f"[EXTRACTOR][DEBUG] Reasoning: {result_data.get('reasoning')}")
        if result_data.get('source_quote'):
            print(f"[EXTRACTOR][DEBUG] Source quote: {result_data.get('source_quote')[:100]}...")
    
    return ExtractedVariable(
        variable_name=task.variable_name,
        value=result_data.get("value"),
        confidence=float(result_data.get("confidence", 0.0)),
        provenance=[p.source_url for p in passages[:max_passages]],
        extraction_method="llm",
        raw_passages=[p.text for p in passages[:max_passages]]
    )

except json.JSONDecodeError:
    # âš ï¸ JSON è§£æå¤±è´¥ â†’ confidence 0.5
    return ExtractedVariable(
        variable_name=task.variable_name,
        value=result_text,  # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬
        confidence=0.5,  # ç¡¬ç¼–ç  0.5
        provenance=[p.source_url for p in passages],
        extraction_method="llm_fallback",
        raw_passages=[p.text for p in passages]
    )
```

---

### ç¬¬ 3 æ­¥Bï¼šFallback æå– - `_fallback_extract()` (å¤‡ç”¨è·¯å¾„)

**ä½ç½®**: ç¬¬ 696-726 è¡Œ

**ä½•æ—¶è§¦å‘**:
1. æ²¡æœ‰ LLM client
2. `_llm_extract()` æŠ›å‡ºå¼‚å¸¸ï¼ˆå¦‚ `'NoneType' object has no attribute 'strip'`ï¼‰

```python
def _fallback_extract(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
    """Fallback extraction using simple heuristics"""
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰ passages
    if not passages:
        return ExtractedVariable(
            variable_name=task.variable_name,
            value=None,
            confidence=0.0,
            extraction_method="fallback"
        )
    
    # 2. ä½¿ç”¨æœ€é«˜åˆ†çš„ passage
    best_passage = passages[0]
    
    # 3. æ ¹æ® category é€‰æ‹©æå–æ–¹æ³•
    if task.category == "biography":
        value = self._extract_biographical_info(best_passage.text)
    elif task.category == "fact_verification":
        value = self._extract_factual_claim(best_passage.text)
    else:
        # Generic extraction - use first sentence or paragraph
        sentences = best_passage.text.split('.')
        value = sentences[0].strip() if sentences else best_passage.text[:200]
    
    # 4. è¿”å›ç»“æœï¼Œç¡¬ç¼–ç  confidence 0.6
    return ExtractedVariable(
        variable_name=task.variable_name,
        value=value,
        confidence=0.6,  # âš ï¸ ç¡¬ç¼–ç  0.6
        provenance=[p.source_url for p in passages],
        extraction_method="fallback",
        raw_passages=[p.text for p in passages]
    )
```

**ä¾‹å­**:
```
task.fact = "éƒ­æ¯…å¯çš„å­¦æœ¯èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ"
task.category = "biography"

best_passage.text = "éƒ­æ¯…å¯æ•™æˆ1985å¹´æ¯•ä¸šäºæ¸…åå¤§å­¦ï¼Œè·å¾—è®¡ç®—æœºç§‘å­¦å­¦å£«å­¦ä½ã€‚1990å¹´åœ¨ç¾å›½æ–¯å¦ç¦å¤§å­¦è·å¾—åšå£«å­¦ä½ã€‚"

â†“ è°ƒç”¨ _extract_biographical_info()

value = "1985å¹´æ¯•ä¸šäºæ¸…åå¤§å­¦"  (åŒ¹é…åˆ° "graduated from" æ¨¡å¼)
confidence = 0.6
```

---

## Confidence å€¼çš„æ¥æºæ€»ç»“

### Confidence æ¥æºè¡¨

| Confidence | æ¥æº | å«ä¹‰ | è§¦å‘æ¡ä»¶ |
|-----------|------|------|---------|
| 0.0-1.0 (LLM) | `_llm_extract()` æˆåŠŸ | LLM è‡ªå·±è¯„ä¼°çš„ç½®ä¿¡åº¦ | JSON è§£ææˆåŠŸ |
| **0.5** | `_llm_extract()` JSON å¤±è´¥ | LLM è¿”å›é JSON æ ¼å¼ | `json.JSONDecodeError` |
| **0.6** | `_fallback_extract()` | ç®€å•å¯å‘å¼æå– | LLM è°ƒç”¨å¼‚å¸¸æˆ–æ—  client |
| 0.0 | `_fallback_extract()` æ—  passages | æ²¡æœ‰å¯ç”¨çš„ passages | `if not passages` |

### ä½ çœ‹åˆ°çš„æ—¥å¿—

```
ğŸ” EXTRACTOR: Extracted 'guoyi_profile' with confidence 0.50
```
**å«ä¹‰**: LLM è¿”å›äº†é JSON æ ¼å¼ï¼Œè§¦å‘äº† `json.JSONDecodeError`

```
Extraction failed for guoyi_qualifications: 'NoneType' object has no attribute 'strip'
ğŸ” EXTRACTOR: Extracted 'guoyi_qualifications' with confidence 0.60
```
**å«ä¹‰**: LLM è¿”å› `None`ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼Œé™çº§åˆ° fallback æå–

```
[EXTRACTOR][DEBUG] Extracted value: None
[EXTRACTOR][DEBUG] Confidence: 0.0
[EXTRACTOR][DEBUG] Reasoning: None of the provided passages mention Hong Kong Polytechnic University...
ğŸ” EXTRACTOR: Extracted 'current_president_end' with confidence 0.00
```
**å«ä¹‰**: LLM æ­£å¸¸å·¥ä½œï¼Œä½†ç¡®å®æ²¡æ‰¾åˆ°ä¿¡æ¯ï¼Œè¯šå®åœ°è¿”å› 0.0

---

## æ ¸å¿ƒé—®é¢˜åˆ†æ

### é—®é¢˜ 1: Prompt è¯­è¨€æ··åˆ

**å½“å‰æƒ…å†µ**:
```
Prompt æŒ‡ä»¤: è‹±æ–‡
QUESTION: éƒ­æ¯…å¯ä»€ä¹ˆæ—¶å€™å¯ä»¥å½“æµ¸ä¼šå¤§å­¦æ ¡é•¿ï¼Ÿ (ä¸­æ–‡)
CORE ENTITIES: éƒ­æ¯…å¯, ä»€ä¹ˆæ—¶å€™, å¯ä»¥, æµ¸ä¼šå¤§å­¦, æ ¡é•¿ (ä¸­æ–‡)
TEXT PASSAGES: éƒ­æ¯…å¯æ•™æˆç°ä»»... (ä¸­æ–‡)
```

**LLM çš„å›°æƒ‘**:
- çœ‹åˆ°è‹±æ–‡æŒ‡ä»¤ + ä¸­æ–‡å†…å®¹
- ä¸ç¡®å®šåº”è¯¥ç”¨ä»€ä¹ˆè¯­è¨€å›ç­”
- å¯èƒ½è¿”å›ä¸­æ–‡æè¿°è€Œä¸æ˜¯ JSON

**å¯¼è‡´**:
```
LLM å“åº”: æ ¹æ®æä¾›çš„æ–‡æœ¬ï¼Œéƒ­æ¯…å¯æ•™æˆç°ä»»...
         (ä¸­æ–‡æè¿°ï¼Œä¸æ˜¯ JSON)

â†“

json.JSONDecodeError

â†“

confidence = 0.5
```

---

### é—®é¢˜ 2: JSON æ ¼å¼çº¦æŸä¸å¼º

**å½“å‰ system message**:
```
"You are an expert information extractor. Extract precise, factual information from text passages."
```

**é—®é¢˜**:
- æ²¡æœ‰æ˜ç¡®è¦æ±‚"å¿…é¡»è¿”å› JSON"
- æ²¡æœ‰å¼ºè°ƒ"ä¸è¦è¿”å›ä»»ä½•å…¶ä»–æ ¼å¼"

**å¯¼è‡´**:
- LLM å¯èƒ½è¿”å›è‡ªç„¶è¯­è¨€
- ç‰¹åˆ«æ˜¯åœ¨çœ‹åˆ°ä¸­æ–‡å†…å®¹æ—¶

---

### é—®é¢˜ 3: é”™è¯¯å¤„ç†ä¸å¤Ÿç»†è‡´

**å½“å‰ä»£ç **:
```python
result_text = response.choices[0].message.content.strip()
```

**é—®é¢˜**:
- å¦‚æœ `content` æ˜¯ `None`ï¼Œä¼šæŠ¥é”™ï¼š`'NoneType' object has no attribute 'strip'`
- æ²¡æœ‰æ£€æŸ¥ `None` çš„æƒ…å†µ

**å¯¼è‡´**:
```
Exception: 'NoneType' object has no attribute 'strip'

â†“

æ•è·å¼‚å¸¸ï¼Œè°ƒç”¨ _fallback_extract()

â†“

confidence = 0.6
```

---

## æ•°æ®æµç¤ºä¾‹

### å®Œæ•´æµç¨‹ç¤ºä¾‹

```
ç”¨æˆ·æŸ¥è¯¢: "éƒ­æ¯…å¯ä»€ä¹ˆæ—¶å€™å¯ä»¥å½“æµ¸ä¼šå¤§å­¦æ ¡é•¿ï¼Ÿ"

â†“ Planner ç”Ÿæˆ tasks

tasks = [
    PlanTask(fact="éƒ­æ¯…å¯çš„å­¦æœ¯èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ", variable_name="guoyi_profile"),
    PlanTask(fact="éƒ­æ¯…å¯ç°ä»»èŒä½æ˜¯ä»€ä¹ˆï¼Ÿ", variable_name="guoyi_current_position"),
    PlanTask(fact="éƒ­æ¯…å¯ä½•æ—¶èƒ½æ‹…ä»»æµ¸ä¼šå¤§å­¦æ ¡é•¿ï¼Ÿ", variable_name="guoyi_appointment_time"),
    ...
]

â†“ Search & Ranking

ranked_passages = {
    "guoyi_profile": [Passage1, Passage2, ..., Passage20],
    "guoyi_current_position": [Passage1, Passage2, ..., Passage20],
    "guoyi_appointment_time": [Passage1, Passage2, ..., Passage20],
    ...
}

â†“ Extractor.extract_variables()

For task "guoyi_profile":
  â”œâ”€ passages = ranked_passages["guoyi_profile"][:20]
  â”œâ”€ _extract_single_variable(task, passages)
  â”‚   â”œâ”€ _llm_extract(task, passages)
  â”‚   â”‚   â”œâ”€ æå–æ ¸å¿ƒå®ä½“: ["éƒ­æ¯…å¯", "å­¦æœ¯", "èƒŒæ™¯"]
  â”‚   â”‚   â”œâ”€ åˆå¹¶ 20 ä¸ª passages
  â”‚   â”‚   â”œâ”€ æ„å»º prompt (è‹±æ–‡æŒ‡ä»¤ + ä¸­æ–‡å†…å®¹)
  â”‚   â”‚   â”œâ”€ è°ƒç”¨ LLM
  â”‚   â”‚   â”œâ”€ LLM è¿”å›: "æ ¹æ®æä¾›çš„æ–‡æœ¬..." (ä¸­æ–‡ï¼Œé JSON)
  â”‚   â”‚   â”œâ”€ json.JSONDecodeError
  â”‚   â”‚   â””â”€ è¿”å› ExtractedVariable(value="æ ¹æ®æä¾›çš„æ–‡æœ¬...", confidence=0.5)
  â”‚   â””â”€ æ‰“å°: confidence 0.50
  â””â”€ extracted_vars["guoyi_profile"] = ExtractedVariable(...)

For task "guoyi_qualifications":
  â”œâ”€ passages = ranked_passages["guoyi_qualifications"][:20]
  â”œâ”€ _extract_single_variable(task, passages)
  â”‚   â”œâ”€ _llm_extract(task, passages)
  â”‚   â”‚   â”œâ”€ è°ƒç”¨ LLM
  â”‚   â”‚   â”œâ”€ LLM è¿”å›: content = None
  â”‚   â”‚   â”œâ”€ result_text = None.strip()  â† Exception!
  â”‚   â”‚   â””â”€ Exception: 'NoneType' object has no attribute 'strip'
  â”‚   â”œâ”€ æ•è·å¼‚å¸¸
  â”‚   â”œâ”€ _fallback_extract(task, passages)
  â”‚   â”‚   â”œâ”€ best_passage = passages[0]
  â”‚   â”‚   â”œâ”€ value = best_passage.text.split('.')[0]
  â”‚   â”‚   â””â”€ è¿”å› ExtractedVariable(value="...", confidence=0.6)
  â”‚   â””â”€ æ‰“å°: "Extraction failed..." + confidence 0.60
  â””â”€ extracted_vars["guoyi_qualifications"] = ExtractedVariable(...)

For task "current_president_end":
  â”œâ”€ passages = ranked_passages["current_president_end"][:20]
  â”œâ”€ _extract_single_variable(task, passages)
  â”‚   â”œâ”€ _llm_extract(task, passages)
  â”‚   â”‚   â”œâ”€ è°ƒç”¨ LLM
  â”‚   â”‚   â”œâ”€ LLM è¿”å›: {"value": null, "confidence": 0.0, "reasoning": "..."}
  â”‚   â”‚   â”œâ”€ JSON è§£ææˆåŠŸ
  â”‚   â”‚   â””â”€ è¿”å› ExtractedVariable(value=None, confidence=0.0)
  â”‚   â””â”€ æ‰“å°: confidence 0.00 (è¿™æ˜¯æ­£å¸¸çš„ï¼)
  â””â”€ extracted_vars["current_president_end"] = ExtractedVariable(...)

â†“ è¿”å›æ‰€æœ‰ extracted_vars

return {
    "guoyi_profile": ExtractedVariable(value="...", confidence=0.5),
    "guoyi_qualifications": ExtractedVariable(value="...", confidence=0.6),
    "current_president_end": ExtractedVariable(value=None, confidence=0.0),
    ...
}
```

---

## æ€»ç»“

### Extractor çš„ä¸‰æ¡è·¯å¾„

1. **æ­£å¸¸è·¯å¾„**: `_llm_extract()` æˆåŠŸ â†’ confidence ç”± LLM å†³å®š (0.0-1.0)
2. **JSON å¤±è´¥è·¯å¾„**: `_llm_extract()` JSON è§£æå¤±è´¥ â†’ confidence 0.5
3. **å¼‚å¸¸è·¯å¾„**: `_llm_extract()` æŠ›å¼‚å¸¸ â†’ `_fallback_extract()` â†’ confidence 0.6

### æ ¸å¿ƒé—®é¢˜

1. **Prompt è¯­è¨€æ··åˆ** - è‹±æ–‡æŒ‡ä»¤ + ä¸­æ–‡å†…å®¹ â†’ LLM å›°æƒ‘ â†’ è¿”å›é JSON
2. **JSON çº¦æŸä¸å¼º** - æ²¡æœ‰æ˜ç¡®è¦æ±‚ JSON only â†’ LLM å¯èƒ½è¿”å›è‡ªç„¶è¯­è¨€
3. **é”™è¯¯å¤„ç†ä¸è¶³** - æ²¡æœ‰æ£€æŸ¥ `None` å“åº” â†’ æŠ›å¼‚å¸¸ â†’ fallback

### ä¸‹ä¸€æ­¥æ”¹è¿›

1. **ç»Ÿä¸€ Prompt è¯­è¨€** - æ£€æµ‹å†…å®¹è¯­è¨€ï¼Œä½¿ç”¨å¯¹åº”è¯­è¨€çš„ prompt
2. **å¼ºåŒ– JSON çº¦æŸ** - ä½¿ç”¨ `response_format={"type": "json_object"}`
3. **æ£€æŸ¥ None å“åº”** - åœ¨ `.strip()` å‰æ£€æŸ¥ `None`

---

æœ€åæ›´æ–°: 2025-10-06
ä½œè€…: AI Assistant
çŠ¶æ€: âœ… è¯¦ç»†åˆ†æå®Œæˆ
