# Extractor æ¿€è¿›æ”¹è¿›æ–¹æ¡ˆ

## å½“å‰é—®é¢˜åˆ†æ

### ä¿¡æ¯æµé‡è®¡ç®—

```
æœç´¢é˜¶æ®µ:
  5 ä¸ªæŸ¥è¯¢ Ã— 10 ä¸ªç»“æœ/æŸ¥è¯¢ = 50 ä¸ªæœç´¢ç»“æœ

ç½‘é¡µæŠ“å–é˜¶æ®µ:
  50 ä¸ª URL Ã— 5 ä¸ªé¡µé¢å®é™…è®¿é—® = 5 ä¸ªå®Œæ•´ç½‘é¡µ
  5 ä¸ªç½‘é¡µ Ã— å¹³å‡ 3000 å­—/é¡µ = 15,000 å­—å†…å®¹

åˆ†å—é˜¶æ®µ:
  15,000 å­— Ã· 500 å­—/chunk = 30 ä¸ª chunks
  è€ƒè™‘ overlapï¼Œå®é™…çº¦ 40-50 ä¸ª passages

Ranking é˜¶æ®µ:
  40-50 ä¸ª passages â†’ æŒ‰ç›¸å…³æ€§æ’åº

Extraction é˜¶æ®µ (å½“å‰):
  æ¯ä¸ª task åªçœ‹å‰ 7 ä¸ª passages
  ä¿¡æ¯åˆ©ç”¨ç‡: 7/45 = 15.6%
  ä¿¡æ¯ä¸¢å¤±ç‡: 84.4%
```

### æ ¸å¿ƒé—®é¢˜

**å³ä½¿æ”¹åˆ° 7 ä¸ª passagesï¼Œä¿¡æ¯ä¸¢å¤±ç‡ä»ç„¶é«˜è¾¾ 84%ï¼**

---

## æ¿€è¿›æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: å¤§å¹…å¢åŠ  passages æ•°é‡

#### A1: å¢åŠ åˆ° 15 ä¸ª

```yaml
max_passages_per_task: 15
```

**æ•ˆæœ**:
- ä¿¡æ¯åˆ©ç”¨ç‡: 15/45 = 33%
- ä¿¡æ¯ä¸¢å¤±ç‡: 67%
- Token æ¶ˆè€—: ~7,500 tokens (15 Ã— 500)

**ä¼˜ç‚¹**:
- ç®€å•ç›´æ¥
- è¦†ç›–æ›´å¤šä¿¡æ¯

**ç¼ºç‚¹**:
- Token æ¶ˆè€—å¢åŠ  2å€
- å¯èƒ½å¼•å…¥å™ªéŸ³

---

#### A2: å¢åŠ åˆ° 20 ä¸ª

```yaml
max_passages_per_task: 20
```

**æ•ˆæœ**:
- ä¿¡æ¯åˆ©ç”¨ç‡: 20/45 = 44%
- ä¿¡æ¯ä¸¢å¤±ç‡: 56%
- Token æ¶ˆè€—: ~10,000 tokens (20 Ã— 500)

**ä¼˜ç‚¹**:
- æ¥è¿‘ä¸€åŠçš„ä¿¡æ¯è¢«åˆ©ç”¨
- æ˜¾è‘—é™ä½ä¸¢å¤±ç‡

**ç¼ºç‚¹**:
- Token æ¶ˆè€—å¢åŠ  3å€
- éœ€è¦æ›´å¥½çš„ prompt æ¥å¤„ç†æ›´å¤šä¿¡æ¯

---

#### A3: åŠ¨æ€è°ƒæ•´ï¼ˆæ¨èï¼‰

```yaml
max_passages_per_task: 20  # é»˜è®¤å€¼

# æ ¹æ® task å¤æ‚åº¦åŠ¨æ€è°ƒæ•´
simple_task_passages: 10
complex_task_passages: 30
```

**é€»è¾‘**:
```python
if task.category == "simple_fact":
    max_passages = 10
elif task.category == "complex_analysis":
    max_passages = 30
else:
    max_passages = 20
```

---

### æ–¹æ¡ˆ B: ä¸¤é˜¶æ®µ Extraction

#### B1: ç²—æå– + ç²¾æå–

```
Stage 1: ç²—æå– (çœ‹æ‰€æœ‰ passages)
  â†’ ä»æ‰€æœ‰ 40-50 ä¸ª passages ä¸­å¿«é€Ÿç­›é€‰
  â†’ æ‰¾å‡ºå¯èƒ½åŒ…å«ç­”æ¡ˆçš„ passages
  â†’ è¾“å‡º: Top 10-15 ä¸ªå€™é€‰ passages

Stage 2: ç²¾æå– (æ·±åº¦åˆ†æ)
  â†’ å¯¹ Top 10-15 ä¸ªå€™é€‰è¿›è¡Œæ·±åº¦åˆ†æ
  â†’ æå–å…·ä½“ä¿¡æ¯
  â†’ éªŒè¯ä¸€è‡´æ€§
  â†’ è¾“å‡º: æœ€ç»ˆç­”æ¡ˆ
```

**ä¼˜ç‚¹**:
- ä¿¡æ¯åˆ©ç”¨ç‡: 100% (Stage 1)
- ç²¾å‡†åº¦é«˜ (Stage 2)
- Token æ¶ˆè€—å¯æ§

**å®ç°**:
```python
# Stage 1: Quick scan (use cheaper model)
candidates = await self._quick_scan_all_passages(task, all_passages)

# Stage 2: Deep extraction (use better model)
final_value = await self._deep_extract(task, candidates)
```

---

### æ–¹æ¡ˆ C: å¤šè½® Extraction + æŠ•ç¥¨

#### C1: åˆ†æ‰¹æå– + æŠ•ç¥¨

```
Round 1: Extract from passages 1-10
  â†’ Value A, Confidence 0.8

Round 2: Extract from passages 11-20
  â†’ Value B, Confidence 0.7

Round 3: Extract from passages 21-30
  â†’ Value A, Confidence 0.9

Final: Vote and merge
  â†’ Value A appears 2/3 times
  â†’ Average confidence: 0.85
  â†’ Final answer: Value A (confidence 0.85)
```

**ä¼˜ç‚¹**:
- ä¿¡æ¯åˆ©ç”¨ç‡: 100%
- å¤šæºéªŒè¯ï¼Œå‡å°‘å¹»è§‰
- å¯ä»¥å‘ç°ä¸ä¸€è‡´çš„ä¿¡æ¯

**ç¼ºç‚¹**:
- éœ€è¦å¤šæ¬¡ LLM è°ƒç”¨
- æ—¶é—´å’Œæˆæœ¬å¢åŠ 

---

### æ–¹æ¡ˆ D: æ”¹è¿› Ranking ç®—æ³•

#### D1: ä½¿ç”¨ Embedding Ranking

```python
# å½“å‰: keyword matching
score = entity_matches * 3.0 + keyword_matches * 1.0

# æ”¹è¿›: semantic similarity
query_embedding = get_embedding(task.fact)
passage_embedding = get_embedding(passage.text)
semantic_score = cosine_similarity(query_embedding, passage_embedding)

# æ··åˆ ranking
final_score = semantic_score * 0.6 + keyword_score * 0.4
```

**æ•ˆæœ**:
- æ›´ç²¾å‡†çš„ ranking
- çœŸæ­£ç›¸å…³çš„ passages æ’åˆ°å‰é¢
- å³ä½¿åªçœ‹å‰ 10 ä¸ªï¼Œä¹Ÿèƒ½è¦†ç›–æœ€é‡è¦çš„ä¿¡æ¯

---

## æ¨èæ–¹æ¡ˆç»„åˆ

### ç»„åˆ 1: æ¿€è¿›ä½†å®ç”¨ï¼ˆæ¨èï¼‰

```yaml
# config.yaml
ir_rag:
  content:
    max_passages_per_task: 20    # ä» 7 å¢åŠ åˆ° 20
  
  ranking:
    entity_weight: 3.5           # ä» 3.0 å¢åŠ åˆ° 3.5
    use_semantic_ranking: false  # æš‚æ—¶ä¿æŒ falseï¼ˆP3 å®ç°ï¼‰
```

**+ ä»£ç æ”¹è¿›**:
- æ”¹è¿› prompt ä»¥å¤„ç†æ›´å¤š passages
- æ·»åŠ  passage å»é‡é€»è¾‘
- ä¼˜åŒ– token ä½¿ç”¨

**é¢„æœŸæ•ˆæœ**:
- ä¿¡æ¯åˆ©ç”¨ç‡: 20/45 = 44%
- ä¿¡æ¯ä¸¢å¤±ç‡: 56%
- å‡†ç¡®æ€§: 70% â†’ 85%

---

### ç»„åˆ 2: æœ€æ¿€è¿›ï¼ˆæœ€ä½³æ•ˆæœï¼‰

```yaml
# config.yaml
ir_rag:
  content:
    max_passages_per_task: 30    # å¢åŠ åˆ° 30
  
  extraction:
    use_two_stage: true          # å¯ç”¨ä¸¤é˜¶æ®µæå–
    stage1_max_passages: 50      # Stage 1 çœ‹æ‰€æœ‰
    stage2_max_passages: 15      # Stage 2 ç²¾æå–
```

**+ å®ç°ä¸¤é˜¶æ®µ Extraction**:
```python
async def _two_stage_extract(self, task, all_passages):
    # Stage 1: Quick scan (ç”¨ä¾¿å®œçš„æ¨¡å‹)
    candidates = await self._quick_scan(task, all_passages[:50])
    
    # Stage 2: Deep extract (ç”¨å¥½çš„æ¨¡å‹)
    final_value = await self._deep_extract(task, candidates[:15])
    
    return final_value
```

**é¢„æœŸæ•ˆæœ**:
- ä¿¡æ¯åˆ©ç”¨ç‡: 100% (Stage 1) + ç²¾å‡†æå– (Stage 2)
- ä¿¡æ¯ä¸¢å¤±ç‡: 0% (Stage 1)
- å‡†ç¡®æ€§: 70% â†’ 90%

---

## ç«‹å³å¯å®æ–½çš„æ”¹è¿›

### ä¿®æ”¹ 1: å¢åŠ åˆ° 20 ä¸ª passages

```yaml
# config/config.yaml
ir_rag:
  content:
    max_passages_per_task: 20  # ä» 7 å¢åŠ åˆ° 20
```

### ä¿®æ”¹ 2: æ”¹è¿› Extraction Prompt

```python
# src/actions/ir_rag.py

# å½“å‰ prompt éœ€è¦ä¼˜åŒ–ä»¥å¤„ç†æ›´å¤š passages
prompt = f"""
You have {len(passages[:max_passages])} text passages to analyze.

**STRATEGY**:
1. First, quickly scan all passages to identify which ones contain relevant information
2. Focus on passages that mention the CORE ENTITIES: {entities_str}
3. If multiple passages provide the same information, note the consensus
4. If passages contradict each other, note the discrepancy and explain

QUESTION: {task.fact}
CORE ENTITIES: {entities_str}

TEXT PASSAGES:
{combined_text}

**CRITICAL REQUIREMENTS**:
1. Extract information ONLY from the provided passages
2. The information MUST relate to the CORE ENTITIES
3. If multiple passages agree, increase confidence
4. If passages disagree, decrease confidence and explain
5. Quote specific passages (e.g., "Source 3 and Source 7 both state...")

Response format:
{{
    "value": "extracted information",
    "confidence": 0.0-1.0,
    "reasoning": "explanation with source references",
    "source_quotes": ["quote from Source 3", "quote from Source 7"],
    "consensus": "high/medium/low"
}}
"""
```

### ä¿®æ”¹ 3: æ·»åŠ  Passage å»é‡

```python
def _deduplicate_passages(self, passages: List[ContentPassage]) -> List[ContentPassage]:
    """Remove duplicate or highly similar passages"""
    unique_passages = []
    seen_content = set()
    
    for passage in passages:
        # Simple deduplication by content hash
        content_hash = hash(passage.text[:200])  # Use first 200 chars
        
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            unique_passages.append(passage)
    
    return unique_passages
```

---

## Token æ¶ˆè€—åˆ†æ

### å½“å‰ (7 passages)
```
7 passages Ã— 500 chars/passage = 3,500 chars
3,500 chars Ã· 4 = ~875 tokens
Prompt overhead: ~300 tokens
Total per extraction: ~1,175 tokens
```

### æ”¹è¿›åˆ° 20 passages
```
20 passages Ã— 500 chars/passage = 10,000 chars
10,000 chars Ã· 4 = ~2,500 tokens
Prompt overhead: ~300 tokens
Total per extraction: ~2,800 tokens
```

**å¢åŠ **: 2.4å€ token æ¶ˆè€—

### æ”¹è¿›åˆ° 30 passages
```
30 passages Ã— 500 chars/passage = 15,000 chars
15,000 chars Ã· 4 = ~3,750 tokens
Prompt overhead: ~300 tokens
Total per extraction: ~4,050 tokens
```

**å¢åŠ **: 3.4å€ token æ¶ˆè€—

---

## æˆæœ¬æ•ˆç›Šåˆ†æ

### åœºæ™¯: æ¯æ¬¡æŸ¥è¯¢ 3 ä¸ª extraction tasks

#### å½“å‰æ–¹æ¡ˆ (7 passages)
```
Cost per query: 3 tasks Ã— 1,175 tokens = 3,525 tokens
Accuracy: 70%
Cost per accurate answer: 3,525 / 0.7 = 5,036 tokens
```

#### æ–¹æ¡ˆ A (20 passages)
```
Cost per query: 3 tasks Ã— 2,800 tokens = 8,400 tokens
Accuracy: 85%
Cost per accurate answer: 8,400 / 0.85 = 9,882 tokens
```

**ç»“è®º**: è™½ç„¶ token å¢åŠ  2.4å€ï¼Œä½†å‡†ç¡®æ€§æå‡ 15%ï¼Œæ¯ä¸ªå‡†ç¡®ç­”æ¡ˆçš„æˆæœ¬åªå¢åŠ  96%

#### æ–¹æ¡ˆ B (30 passages)
```
Cost per query: 3 tasks Ã— 4,050 tokens = 12,150 tokens
Accuracy: 90%
Cost per accurate answer: 12,150 / 0.9 = 13,500 tokens
```

**ç»“è®º**: Token å¢åŠ  3.4å€ï¼Œå‡†ç¡®æ€§æå‡ 20%ï¼Œæ¯ä¸ªå‡†ç¡®ç­”æ¡ˆçš„æˆæœ¬å¢åŠ  168%

---

## å»ºè®®

### ç«‹å³å®æ–½ï¼ˆä»Šå¤©ï¼‰

1. **å¢åŠ  `max_passages_per_task` åˆ° 20**
   ```yaml
   max_passages_per_task: 20
   ```

2. **æ”¹è¿› Extraction Prompt**
   - æ·»åŠ å¤„ç†å¤š passages çš„ç­–ç•¥
   - æ·»åŠ ä¸€è‡´æ€§æ£€æŸ¥
   - æ·»åŠ  `consensus` å­—æ®µ

3. **æ·»åŠ  Passage å»é‡**
   - é¿å…é‡å¤å†…å®¹æµªè´¹ tokens

**é¢„æœŸæ•ˆæœ**:
- ä¿¡æ¯ä¸¢å¤±ç‡: 84% â†’ 56%
- å‡†ç¡®æ€§: 70% â†’ 85%
- Token æ¶ˆè€—: +140%

### æœ¬å‘¨å®æ–½

4. **å®ç°ä¸¤é˜¶æ®µ Extraction**
   - Stage 1: å¿«é€Ÿæ‰«ææ‰€æœ‰ passages
   - Stage 2: æ·±åº¦æå–å€™é€‰ passages

5. **æ·»åŠ å¤šæºéªŒè¯**
   - æ£€æŸ¥å¤šä¸ª passages çš„ä¸€è‡´æ€§
   - æ ‡æ³¨ consensus level

**é¢„æœŸæ•ˆæœ**:
- ä¿¡æ¯ä¸¢å¤±ç‡: 56% â†’ 0%
- å‡†ç¡®æ€§: 85% â†’ 90%

### ä¸‹å‘¨å®æ–½

6. **å‡çº§ Ranking ç®—æ³•**
   - å®ç° semantic similarity ranking
   - æ··åˆ keyword + semantic

**é¢„æœŸæ•ˆæœ**:
- æ›´ç²¾å‡†çš„ ranking
- å³ä½¿ passages æ•°é‡ä¸å˜ï¼Œå‡†ç¡®æ€§ä¹Ÿæå‡

---

## æ€»ç»“

**å½“å‰æœ€å¤§çš„é—®é¢˜**: 7 ä¸ª passages è¿œè¿œä¸å¤Ÿï¼Œä¿¡æ¯ä¸¢å¤±ç‡ 84% å¤ªé«˜

**ç«‹å³å¯åšçš„æ”¹è¿›**:
1. å¢åŠ åˆ° 20 ä¸ª passages (ä¿¡æ¯ä¸¢å¤±ç‡é™åˆ° 56%)
2. æ”¹è¿› prompt å¤„ç†æ›´å¤šä¿¡æ¯
3. æ·»åŠ å»é‡é¿å…æµªè´¹

**æ•ˆæœ**:
- å‡†ç¡®æ€§: 70% â†’ 85% (+15%)
- Token æˆæœ¬: +140%
- æ€§ä»·æ¯”: å¯æ¥å—

**ä¸‹ä¸€æ­¥**: å®ç°ä¸¤é˜¶æ®µ Extractionï¼Œè¾¾åˆ° 90% å‡†ç¡®æ€§

---

æœ€åæ›´æ–°: 2025-10-06
ä½œè€…: AI Assistant
çŠ¶æ€: ğŸ“‹ æ–¹æ¡ˆå·²åˆ¶å®šï¼Œå¾…å®æ–½
