# Extractor Confidence 0.5 é—®é¢˜åˆ†æ

## ç»ˆç«¯æ—¥å¿—åˆ†æ

```
ğŸ” EXTRACTOR: Extracted 'guoyi_profile' with confidence 0.50
ğŸ” EXTRACTOR: Extracted 'guoyi_current_position' with confidence 0.50
Extraction failed for guoyi_qualifications: 'NoneType' object has no attribute 'strip'
ğŸ” EXTRACTOR: Extracted 'guoyi_qualifications' with confidence 0.60
ğŸ” EXTRACTOR: Extracted 'guoyi_interest_statement' with confidence 0.50
ğŸ” EXTRACTOR: Extracted 'current_president_name' with confidence 0.50
Extraction failed for current_president_start: 'NoneType' object has no attribute 'strip'
ğŸ” EXTRACTOR: Extracted 'current_president_start' with confidence 0.60
[EXTRACTOR][DEBUG] Extracted value: None
[EXTRACTOR][DEBUG] Confidence: 0.0
[EXTRACTOR][DEBUG] Reasoning: None of the provided passages mention Hong Kong Polytechnic University or its current president, so the requested information cannot be extracted.
ğŸ” EXTRACTOR: Extracted 'current_president_end' with confidence 0.00
```

---

## é—®é¢˜ 1: Confidence 0.5 å’Œ 0.6 çš„å«ä¹‰

### Confidence 0.5 çš„æ¥æº

æŸ¥çœ‹ä»£ç  `src/actions/ir_rag.py` ç¬¬ 685-694 è¡Œï¼š

```python
except json.JSONDecodeError:
    # Fallback if JSON parsing fails
    return ExtractedVariable(
        variable_name=task.variable_name,
        value=result_text,
        confidence=0.5,  # âš ï¸ ç¡¬ç¼–ç çš„ 0.5
        provenance=[p.source_url for p in passages],
        extraction_method="llm_fallback",
        raw_passages=[p.text for p in passages]
    )
```

**å«ä¹‰**: 
- LLM è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„ JSON
- ç³»ç»Ÿä½¿ç”¨ fallback æœºåˆ¶
- **ç¡¬ç¼–ç  confidence ä¸º 0.5**

### Confidence 0.6 çš„æ¥æº

æŸ¥çœ‹ä»£ç  `src/actions/ir_rag.py` ç¬¬ 696-726 è¡Œï¼š

```python
def _fallback_extract(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
    """Fallback extraction using simple heuristics"""
    if not passages:
        return ExtractedVariable(
            variable_name=task.variable_name,
            value=None,
            confidence=0.0,
            extraction_method="fallback"
        )
    
    # Use the highest-scored passage as the answer
    best_passage = passages[0]
    
    # Simple extraction based on task category
    if task.category == "biography":
        value = self._extract_biographical_info(best_passage.text)
    elif task.category == "fact_verification":
        value = self._extract_factual_claim(best_passage.text)
    else:
        # Generic extraction - use first sentence or paragraph
        sentences = best_passage.text.split('.')
        value = sentences[0].strip() if sentences else best_passage.text[:200]
    
    return ExtractedVariable(
        variable_name=task.variable_name,
        value=value,
        confidence=0.6,  # âš ï¸ ç¡¬ç¼–ç çš„ 0.6
        provenance=[p.source_url for p in passages],
        extraction_method="fallback",
        raw_passages=[p.text for p in passages]
    )
```

**å«ä¹‰**:
- LLM extraction å¤±è´¥ï¼ˆæŠ›å‡ºå¼‚å¸¸ï¼‰
- ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•æå–
- ä»ç¬¬ä¸€ä¸ª passage æå–ç¬¬ä¸€å¥è¯
- **ç¡¬ç¼–ç  confidence ä¸º 0.6**

---

## é—®é¢˜ 2: ä¸ºä»€ä¹ˆä¼šè§¦å‘ Fallbackï¼Ÿ

### åŸå›  A: JSON è§£æå¤±è´¥ (confidence 0.5)

**å¯èƒ½çš„æƒ…å†µ**:

1. **LLM è¿”å›æ ¼å¼ä¸æ­£ç¡®**
   ```
   æœŸæœ›: {"value": "...", "confidence": 0.8, ...}
   å®é™…: The extracted information is...
   ```

2. **LLM è¿”å›åŒ…å«é¢å¤–æ–‡æœ¬**
   ```
   æœŸæœ›: {"value": "...", "confidence": 0.8, ...}
   å®é™…: Here is the extracted information:
         {"value": "...", "confidence": 0.8, ...}
   ```

3. **LLM è¿”å›ä¸å®Œæ•´çš„ JSON**
   ```
   æœŸæœ›: {"value": "...", "confidence": 0.8, "reasoning": "..."}
   å®é™…: {"value": "...", "confidence": 0.8  (truncated)
   ```

### åŸå›  B: LLM è°ƒç”¨å¼‚å¸¸ (confidence 0.6)

**å¯èƒ½çš„æƒ…å†µ**:

1. **`'NoneType' object has no attribute 'strip'` é”™è¯¯**
   ```python
   result_text = response.choices[0].message.content.strip()
   ```
   å¦‚æœ `content` æ˜¯ `None`ï¼Œå°±ä¼šæŠ¥è¿™ä¸ªé”™è¯¯

2. **LLM è¿”å›ç©ºå“åº”**
   - `response.choices[0].message.content` æ˜¯ `None`
   - å¯èƒ½æ˜¯ API é—®é¢˜æˆ– token é™åˆ¶

3. **å…¶ä»–å¼‚å¸¸**
   - ç½‘ç»œè¶…æ—¶
   - API é™æµ
   - æ¨¡å‹é”™è¯¯

---

## é—®é¢˜ 3: ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡æŠ½å–ï¼Ÿ

### æŸ¥çœ‹ Extraction Prompt

ä»ä»£ç ç¬¬ 598-652 è¡Œå¯ä»¥çœ‹åˆ° promptï¼š

```python
prompt = f"""
Extract the following information from the provided text passages:

QUESTION: {task.fact}
VARIABLE: {task.variable_name}
CATEGORY: {task.category}
CORE ENTITIES: {entities_str}

TEXT PASSAGES:
{combined_text}

**CRITICAL REQUIREMENTS**:
1. Extract information ONLY from the provided text passages above
2. DO NOT use your own knowledge or make assumptions
3. The extracted information MUST mention or relate to the CORE ENTITIES listed above
4. If multiple passages provide different information, prioritize the most specific and recent one
5. Quote the original text when possible to support your extraction

Please provide a JSON response with:
{{
    "value": "extracted information (or null if not found)",
    "confidence": 0.0-1.0,
    "reasoning": "brief explanation with source reference (e.g., 'Source 2 states...')",
    "source_quote": "direct quote from the passage supporting this extraction"
}}

If the information is not found in the passages, set confidence to 0.0 and value to null.
If the information does not relate to the CORE ENTITIES, set confidence to 0.0 and value to null.
"""
```

**é—®é¢˜**:
- âŒ Prompt æ˜¯**å…¨è‹±æ–‡**çš„
- âŒ ä½† `QUESTION` å’Œ `TEXT PASSAGES` å¯èƒ½æ˜¯**ä¸­æ–‡**
- âŒ è¿™ç§æ··åˆå¯èƒ½å¯¼è‡´ LLM å›°æƒ‘

**ä¾‹å­**:
```
QUESTION: éƒ­æ¯…å¯ä»€ä¹ˆæ—¶å€™å¯ä»¥å½“æµ¸ä¼šå¤§å­¦æ ¡é•¿ï¼Ÿ  (ä¸­æ–‡)
CORE ENTITIES: éƒ­æ¯…å¯, æµ¸ä¼šå¤§å­¦, æ ¡é•¿  (ä¸­æ–‡)

TEXT PASSAGES:
Source 1: https://...
éƒ­æ¯…å¯æ•™æˆç°ä»»...  (ä¸­æ–‡å†…å®¹)

**CRITICAL REQUIREMENTS**:  (è‹±æ–‡æŒ‡ä»¤)
1. Extract information ONLY from...
```

**LLM å¯èƒ½çš„å›°æƒ‘**:
- åº”è¯¥ç”¨ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡å›ç­”ï¼Ÿ
- JSON çš„ value åº”è¯¥æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡ï¼Ÿ
- å¯¼è‡´è¿”å›æ ¼å¼ä¸ç¨³å®š

---

## é—®é¢˜ 4: å…·ä½“æ¡ˆä¾‹åˆ†æ

### æ¡ˆä¾‹ 1: guoyi_profile (confidence 0.5)

```
ğŸ” EXTRACTOR: Extracted 'guoyi_profile' with confidence 0.50
```

**æ¨æµ‹**:
- LLM è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆ JSON
- å¯èƒ½è¿”å›äº†ä¸­æ–‡æè¿°è€Œä¸æ˜¯ JSON æ ¼å¼
- è§¦å‘ `json.JSONDecodeError`
- ä½¿ç”¨ fallbackï¼Œconfidence è®¾ä¸º 0.5

**å¯èƒ½çš„ LLM å“åº”**:
```
éƒ­æ¯…å¯æ•™æˆçš„ç®€ä»‹å¦‚ä¸‹ï¼š...
```
è€Œä¸æ˜¯:
```json
{"value": "éƒ­æ¯…å¯æ•™æˆçš„ç®€ä»‹å¦‚ä¸‹ï¼š...", "confidence": 0.8, ...}
```

---

### æ¡ˆä¾‹ 2: guoyi_qualifications (confidence 0.6)

```
Extraction failed for guoyi_qualifications: 'NoneType' object has no attribute 'strip'
ğŸ” EXTRACTOR: Extracted 'guoyi_qualifications' with confidence 0.60
```

**æ¨æµ‹**:
- LLM è¿”å›çš„ `content` æ˜¯ `None`
- è§¦å‘ `'NoneType' object has no attribute 'strip'` é”™è¯¯
- æ•è·å¼‚å¸¸ï¼Œè°ƒç”¨ `_fallback_extract()`
- ä½¿ç”¨ç®€å•å¯å‘å¼æå–ï¼Œconfidence è®¾ä¸º 0.6

**å¯èƒ½çš„åŸå› **:
- API è¿”å›ç©ºå“åº”
- Token é™åˆ¶å¯¼è‡´æˆªæ–­
- æ¨¡å‹å†…éƒ¨é”™è¯¯

---

### æ¡ˆä¾‹ 3: current_president_end (confidence 0.0)

```
[EXTRACTOR][DEBUG] Extracted value: None
[EXTRACTOR][DEBUG] Confidence: 0.0
[EXTRACTOR][DEBUG] Reasoning: None of the provided passages mention Hong Kong Polytechnic University or its current president, so the requested information cannot be extracted.
ğŸ” EXTRACTOR: Extracted 'current_president_end' with confidence 0.00
```

**æ¨æµ‹**:
- è¿™ä¸ªæ˜¯**æ­£å¸¸æƒ…å†µ**
- LLM æ­£ç¡®è¿”å›äº† JSON
- ä½†ä¿¡æ¯ç¡®å®ä¸å­˜åœ¨äº passages ä¸­
- LLM è¯šå®åœ°è¿”å› `value: None, confidence: 0.0`

**è¿™æ˜¯å¥½çš„è¡Œä¸ºï¼**

---

## æ ¹æœ¬é—®é¢˜æ€»ç»“

### 1. Prompt è¯­è¨€ä¸ä¸€è‡´

**é—®é¢˜**:
- Prompt æŒ‡ä»¤æ˜¯è‹±æ–‡
- Question å’Œ Passages æ˜¯ä¸­æ–‡
- LLM å¯èƒ½å›°æƒ‘åº”è¯¥ç”¨ä»€ä¹ˆè¯­è¨€å›ç­”

**å½±å“**:
- JSON æ ¼å¼ä¸ç¨³å®š
- è§¦å‘ fallback æœºåˆ¶
- Confidence é™ä½åˆ° 0.5 æˆ– 0.6

---

### 2. JSON æ ¼å¼çº¦æŸä¸å¤Ÿå¼º

**é—®é¢˜**:
- æ²¡æœ‰æ˜ç¡®è¦æ±‚"å¿…é¡»è¿”å› JSON"
- æ²¡æœ‰æä¾› JSON ç¤ºä¾‹
- LLM å¯èƒ½è¿”å›è‡ªç„¶è¯­è¨€æè¿°

**å½±å“**:
- JSON è§£æå¤±è´¥
- è§¦å‘ fallback

---

### 3. é”™è¯¯å¤„ç†ä¸å¤Ÿç»†è‡´

**é—®é¢˜**:
- `'NoneType' object has no attribute 'strip'` é”™è¯¯
- è¯´æ˜ `response.choices[0].message.content` æ˜¯ `None`
- ä½†ä»£ç æ²¡æœ‰æ£€æŸ¥ `None` çš„æƒ…å†µ

**å½±å“**:
- æŠ›å‡ºå¼‚å¸¸
- è§¦å‘ fallback
- Confidence é™ä½åˆ° 0.6

---

## æ”¹è¿›å»ºè®®

### ç«‹å³æ”¹è¿› 1: ç»Ÿä¸€ Prompt è¯­è¨€

**æ£€æµ‹ Question è¯­è¨€ï¼ŒåŠ¨æ€è°ƒæ•´ Prompt**:

```python
def _detect_language(self, text: str) -> str:
    """Detect if text is primarily Chinese or English"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(text)
    return "zh" if chinese_chars / total_chars > 0.3 else "en"

async def _llm_extract(self, task: PlanTask, passages: List[ContentPassage]) -> ExtractedVariable:
    # Detect language
    lang = self._detect_language(task.fact)
    
    if lang == "zh":
        # Use Chinese prompt
        prompt = f"""
        ä»æä¾›çš„æ–‡æœ¬æ®µè½ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
        
        é—®é¢˜: {task.fact}
        å˜é‡å: {task.variable_name}
        ç±»åˆ«: {task.category}
        æ ¸å¿ƒå®ä½“: {entities_str}
        
        æ–‡æœ¬æ®µè½:
        {combined_text}
        
        **å…³é”®è¦æ±‚**:
        1. ä»…ä»æä¾›çš„æ–‡æœ¬æ®µè½ä¸­æå–ä¿¡æ¯
        2. ä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†æˆ–åšå‡è®¾
        3. æå–çš„ä¿¡æ¯å¿…é¡»ä¸æ ¸å¿ƒå®ä½“ç›¸å…³
        4. å¦‚æœå¤šä¸ªæ®µè½æä¾›ä¸åŒä¿¡æ¯ï¼Œä¼˜å…ˆé€‰æ‹©æœ€å…·ä½“å’Œæœ€æ–°çš„
        5. å°½å¯èƒ½å¼•ç”¨åŸæ–‡æ”¯æŒä½ çš„æå–
        
        è¯·æä¾› JSON æ ¼å¼çš„å“åº”ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSONï¼‰:
        {{
            "value": "æå–çš„ä¿¡æ¯ï¼ˆå¦‚æœæœªæ‰¾åˆ°åˆ™ä¸º nullï¼‰",
            "confidence": 0.0-1.0,
            "reasoning": "ç®€è¦è§£é‡Šå¹¶æ³¨æ˜æ¥æºï¼ˆä¾‹å¦‚ï¼š'æ¥æº 2 æŒ‡å‡º...'ï¼‰",
            "source_quote": "æ”¯æŒæ­¤æå–çš„åŸæ–‡å¼•ç”¨"
        }}
        
        å¦‚æœåœ¨æ®µè½ä¸­æœªæ‰¾åˆ°ä¿¡æ¯ï¼Œè®¾ç½® confidence ä¸º 0.0ï¼Œvalue ä¸º nullã€‚
        å¦‚æœä¿¡æ¯ä¸æ ¸å¿ƒå®ä½“æ— å…³ï¼Œè®¾ç½® confidence ä¸º 0.0ï¼Œvalue ä¸º nullã€‚
        """
    else:
        # Use English prompt (current)
        prompt = f"""..."""
```

---

### ç«‹å³æ”¹è¿› 2: å¼ºåŒ– JSON æ ¼å¼çº¦æŸ

```python
messages=[
    {
        "role": "system", 
        "content": "You are an expert information extractor. You MUST respond with valid JSON only. Do not include any text before or after the JSON."
    },
    {
        "role": "user", 
        "content": prompt
    }
],
response_format={"type": "json_object"}  # OpenAI API çš„ JSON æ¨¡å¼
```

---

### ç«‹å³æ”¹è¿› 3: æ£€æŸ¥ None å“åº”

```python
result_text = response.choices[0].message.content

# Check for None response
if result_text is None:
    print(f"[EXTRACTOR][WARN] LLM returned None for {task.variable_name}")
    return self._fallback_extract(task, passages)

result_text = result_text.strip()
```

---

### ç«‹å³æ”¹è¿› 4: æ›´å¥½çš„ Fallback æ—¥å¿—

```python
except json.JSONDecodeError as e:
    print(f"[EXTRACTOR][WARN] JSON parse failed for {task.variable_name}: {e}")
    print(f"[EXTRACTOR][WARN] Raw response (first 200 chars): {result_text[:200]}")
    
    # Fallback if JSON parsing fails
    return ExtractedVariable(
        variable_name=task.variable_name,
        value=result_text,
        confidence=0.5,
        provenance=[p.source_url for p in passages],
        extraction_method="llm_fallback",
        raw_passages=[p.text for p in passages]
    )
```

---

## æ€»ç»“

### Confidence 0.5 çš„å«ä¹‰
- LLM è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆ JSON
- å¯èƒ½æ˜¯å› ä¸º prompt è¯­è¨€æ··åˆå¯¼è‡´ LLM å›°æƒ‘
- ä½¿ç”¨ fallback æœºåˆ¶ï¼Œç¡¬ç¼–ç  confidence 0.5

### Confidence 0.6 çš„å«ä¹‰
- LLM è°ƒç”¨å¤±è´¥ï¼ˆè¿”å› None æˆ–å¼‚å¸¸ï¼‰
- ä½¿ç”¨ç®€å•å¯å‘å¼æå–ï¼ˆç¬¬ä¸€å¥è¯ï¼‰
- ç¡¬ç¼–ç  confidence 0.6

### è¯­è¨€é—®é¢˜
- **å½“å‰**: Prompt æ˜¯è‹±æ–‡ï¼Œä½† Question å’Œ Passages æ˜¯ä¸­æ–‡
- **é—®é¢˜**: LLM å¯èƒ½å›°æƒ‘ï¼Œè¿”å›æ ¼å¼ä¸ç¨³å®š
- **å»ºè®®**: æ ¹æ® Question è¯­è¨€åŠ¨æ€è°ƒæ•´ Prompt è¯­è¨€

### ä¼˜å…ˆçº§
1. **P0**: æ£€æŸ¥ None å“åº”ï¼ˆé¿å… crashï¼‰
2. **P1**: ç»Ÿä¸€ Prompt è¯­è¨€ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
3. **P1**: å¼ºåŒ– JSON æ ¼å¼çº¦æŸï¼ˆå‡å°‘ fallbackï¼‰
4. **P2**: æ›´å¥½çš„ fallback æ—¥å¿—ï¼ˆä¾¿äºè°ƒè¯•ï¼‰

---

æœ€åæ›´æ–°: 2025-10-06
ä½œè€…: AI Assistant
çŠ¶æ€: âœ… åˆ†æå®Œæˆ
